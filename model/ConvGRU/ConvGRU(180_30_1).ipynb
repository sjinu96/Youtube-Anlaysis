{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T03:17:33.191705Z",
     "start_time": "2020-11-20T03:17:33.186707Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, date\n",
    "from sklearn import utils\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as fn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def _fit_transform(self, raw):\n",
    "        result = raw.copy()\n",
    "\n",
    "        result = self._n_comment_to_float(result)\n",
    "        result = self._str_to_datetype(result)\n",
    "        result = self._add_n_hashtag(result)\n",
    "        \n",
    "        self.non_numeric = ['channel', 'title', 'genre', 'description', 'date', 'sign_in']\n",
    "        result = self._merge(result, self.non_numeric)\n",
    "        \n",
    "        features = ['cumul_view', 'n_dislike', 'n_like', 'n_comment', 'video_n_view', 'cumul_subs']\n",
    "        new_name = ['view_diff', 'dislike_diff', 'like_diff', 'comment_diff', 'video_n_view_diff', 'sub_diff']\n",
    "        result = self._add_diff(result, features, new_name)\n",
    "        \n",
    "        result = self._add_no_upload_interval(result)\n",
    "        result = self._remove_nan(result)\n",
    "        self._one_hot(result)\n",
    "\n",
    "        return result\n",
    "        \n",
    "        \n",
    "        \n",
    "    #FEATRUES TO ADD & MODIFY\n",
    "    ####################################################################     \n",
    "    def _n_comment_to_float(self,result):\n",
    "        idx1 = result['n_comment'] == '댓글 사용 중지'\n",
    "        idx2 = result.n_comment.isna()\n",
    "        idx = idx1|idx2\n",
    "        result['n_comment'].loc[idx] = result['n_comment'].loc[idx].apply(lambda x: 0)\n",
    "        result['n_comment'] = result['n_comment'].astype(float)\n",
    "        return result\n",
    "        \n",
    "    \n",
    "    def _str_to_datetype(self,result):\n",
    "        if pd.api.types.is_datetime64_ns_dtype(result['date']):\n",
    "            pass\n",
    "        else:\n",
    "            result['date'] = pd.to_datetime(result['date'])\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def _add_n_hashtag(self,result):\n",
    "        result['n_hashtage'] = 0\n",
    "        idx = result['description'].notnull()\n",
    "        result.loc[idx, 'n_hashtage'] = result.loc[idx, 'description'].apply(lambda x: len(x.split('#'))-1)\n",
    "        return result\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_to_merge(data, numeric, non_numeric):\n",
    "        data = data.reset_index(drop=True)\n",
    "        num_to_add = data.title.shape[0] - data.title.isna().sum()\n",
    "        data = pd.concat((data.loc[0,non_numeric], data[numeric].mean()))\n",
    "        data['video_num'] = num_to_add\n",
    "        return data\n",
    "    def _merge(self, result, non_numeric):\n",
    "        #operate both merge and creating video_num featrue simultaneously.\n",
    "        numeric = [col for col in result.columns.tolist() if col not in non_numeric]\n",
    "        return result.groupby(['channel', 'date']).apply(lambda x: self._get_to_merge(x, numeric, non_numeric)).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_diff(result, feature, new_name):\n",
    "        result = result.reset_index(drop=True)\n",
    "        result[new_name] = (result[feature] - result[feature].shift())\n",
    "        return result\n",
    "    def _add_diff(self, result, feature, new_name):\n",
    "        result = result.groupby('channel').apply(lambda x: self._get_diff(x, feature, new_name)).reset_index(drop=True)\n",
    "        result[new_name] = result[new_name].fillna(0)\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_no_upload_interval(result):\n",
    "        result = result.reset_index(drop=True)\n",
    "        upload_idx = result[result['video_num'] != 0].index.tolist()\n",
    "        temp = [0 for i in range(result.shape[0])]\n",
    "        for i in range(len(upload_idx)):\n",
    "            if i == len(upload_idx)-1:\n",
    "                former = upload_idx[i]\n",
    "                temp[former+1:] = [i+1 for i in range(len(temp[former+1:]))]\n",
    "            else:\n",
    "                former, latter = upload_idx[i], upload_idx[i+1]\n",
    "                temp[former+1:latter] = [i+1 for i in range(len(temp[former+1:latter]))]\n",
    "        result['no_upload_interval'] = temp\n",
    "        return result\n",
    "    def _add_no_upload_interval(self,result):\n",
    "        return result.groupby('channel').apply(lambda x: self._get_no_upload_interval(x)).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    def _remove_nan(self, result):\n",
    "        numeric = [col for col in result.columns.tolist() if col not in self.non_numeric]\n",
    "        result.loc[:, numeric] = result.loc[:,numeric].fillna(0)\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def _one_hot(self, data):\n",
    "        data.loc[:,'genre'] = data.genre.fillna('etc')\n",
    "        genre = data.genre.unique().tolist()\n",
    "        for i, name in enumerate(genre):\n",
    "            data.genre[data.genre==name] = data.genre[data.genre==name].apply(lambda x: i)\n",
    "            \n",
    "        one_hot = pd.get_dummies(data.genre.unique().tolist())\n",
    "        data['one_hot'] = data.genre\n",
    "        for i in range(len(one_hot)):\n",
    "            data.loc[data.genre==i,'one_hot'] = data.loc[data.genre==i, 'genre'].apply(lambda x: one_hot[i].values)\n",
    "    ####################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    #CREATE SEQUENTIAL DATA\n",
    "    ####################################################################\n",
    "    def _extract_at_least_filter(self, result, filter_size):\n",
    "        #fillter_size 이상인 채널 추출하기\n",
    "        alive_idx = result['channel'].value_counts() >= filter_size\n",
    "        alive_array = alive_idx[alive_idx==True].index\n",
    "        return result[result['channel'].isin(alive_array)].reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _to_sequential(result, filter_size, target_size, stride, features, target_features):\n",
    "        result = result.reset_index(drop=True)\n",
    "        idx_list = result.index.tolist()\n",
    "        \n",
    "        train, target = [],[]\n",
    "        for i in range((len(idx_list)-filter_size-target_size)//stride +1):\n",
    "            train_idx = idx_list[i*stride : i*stride + filter_size]\n",
    "            target_idx = idx_list[i*stride + filter_size : i*stride + filter_size + target_size]\n",
    "            train_temp = result.loc[train_idx,:].values.reshape(1,-1)\n",
    "            target_temp = result.loc[target_idx,target_features].values.reshape(1,-1)\n",
    "            \n",
    "            train = train_temp.copy() if i == 0 else np.vstack([train, train_temp])\n",
    "            target = target_temp.copy() if i == 0 else np.vstack([target, target_temp])\n",
    "            \n",
    "        train = pd.DataFrame(train, columns = result.columns.tolist()*filter_size)\n",
    "        target = pd.DataFrame(target, columns = target_features*target_size)\n",
    "        return train[features], target\n",
    "    def _create_sequential_data(self, result, filter_size=7, target_size=1, stride=1, features=None, target_features=None):\n",
    "        #remove channels with few information with respect to filter_size and target_size to extract\n",
    "        result = self._extract_at_least_filter(result, filter_size + target_size)\n",
    "        \n",
    "        #features: features to drop fromf X (features)\n",
    "        #target_features: features to extract from Y (targets)\n",
    "        if features is None:\n",
    "            features = ['date', 'genre','title', 'channel', 'description',\t'sign_in', 'current_cumul_view', 'current_n_video', 'current_cumul_subs']\n",
    "        if target_features is None:\n",
    "            target_features = ['sub_diff']\n",
    "        \n",
    "        #return train, target set wrt groups\n",
    "        result = result.groupby('channel').apply(lambda x: self._to_sequential(x, filter_size, target_size, stride, features, target_features)).reset_index(drop=True)\n",
    "        return self._combine(result)\n",
    "    \n",
    "    \n",
    "    def _combine(self, result):\n",
    "        temp0, temp1 = [], []\n",
    "        for i in range(len(result)):\n",
    "            temp0.append(result[i][0])\n",
    "            temp1.append(result[i][1])\n",
    "        temp0 = pd.concat(temp0)\n",
    "        temp1 = pd.concat(temp1)\n",
    "        return (temp0, temp1)\n",
    "    ####################################################################\n",
    "\n",
    "\n",
    "    \n",
    "    #SCALE\n",
    "    ####################################################################\n",
    "    def scale(self, data, return_original_scale=True):\n",
    "        original_scale = pd.concat((data.max(), data.min()), axis=1).T\n",
    "        original_scale.index=['max', 'min']\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "        data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n",
    "        if return_original_scale:\n",
    "            return data, original_scale\n",
    "        return data\n",
    "    \n",
    "        \n",
    "    def inverse_scale(self, pred, scl):\n",
    "        for idx in range(pred.shape[1]):\n",
    "            pred.iloc[:,idx] = (scl.iloc[0,idx]-scl.iloc[1,idx])*pred.iloc[:,idx]+scl.iloc[1,idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader(Preprocessor):\n",
    "    def __init__(self, path):\n",
    "        self._raw = pd.read_csv(path)\n",
    "        self.data = super()._fit_transform(self._raw.copy())\n",
    "        print('Data Loaded. :P')\n",
    "        \n",
    "        \n",
    "    def get_data(self, filter_size=7, target_size=1, stride=1, features=None, target_features=None,\n",
    "                         channel:list=None, shuffle=False, random_state=None, order=None):\n",
    "        if channel is None:\n",
    "            data_to_extract = self.data\n",
    "        else:\n",
    "            channels = self.list_channel[channel].tolist()\n",
    "            data_to_extract = self.data.set_index('channel').loc[channels].reset_index()\n",
    "            \n",
    "        train, target = self._create_sequential_data(data_to_extract, filter_size, target_size, stride, features, target_features)\n",
    "        if shuffle:\n",
    "            train, target = utils.shuffle(train, target, random_state=random_state)\n",
    "        \n",
    "        if order:\n",
    "            train_col = [col for col in train.columns.unique().tolist()]\n",
    "            target_col = [col for col in target.columns.unique().tolist()]\n",
    "            train, target = train[train_col], target[target_col]\n",
    "            \n",
    "        return train, target\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def list_features(self):\n",
    "        #list the entire features, hence you can choose which features are included in whole set.\n",
    "        return self.data.columns.tolist()\n",
    "    \n",
    "    @property\n",
    "    def list_channel(self):\n",
    "        #list indices of channel.\n",
    "        return pd.Series(self.data.channel.unique().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> load함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(\n",
    "    filter_size: '60, 90, 180', \n",
    "    target_size: '1, 7, 30, 180', \n",
    "    stride: '1, 2, 3',\n",
    "    drop_suffix: '각 변수 끝에 붙은 번호를 제거할지 여부'=True,\n",
    "    path='/home/mskang/CapstoneUOS/notebooks/ModelResearch_iloveslowfood/data_variants'):\n",
    "    \n",
    "    print(f'Setting: filter_size({filter_size})\\ttarget_size({target_size})\\tstride({stride})\\tdrop_suffix({drop_suffix})')\n",
    "    X_name = f'fs({filter_size})_ts({target_size})_st({stride}).csv'\n",
    "    y_name = f'fs({filter_size})_ts({target_size})_st({stride})_label.csv'\n",
    "    \n",
    "    print('Load feature data...', end='\\t')\n",
    "    X = pd.read_csv(os.path.join(path, X_name))\n",
    "    print('loaded!')\n",
    "    print('Load label data...', end='\\t')\n",
    "    y = pd.read_csv(os.path.join(path, y_name))\n",
    "    print('loaded!')\n",
    "    \n",
    "    if drop_suffix:\n",
    "        X.columns = list(map(lambda x: x.split('.')[0], X.columns.tolist()))\n",
    "        y.columns = list(map(lambda x: x.split('.')[0], y.columns.tolist()))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting: filter_size(180)\ttarget_size(30)\tstride(1)\tdrop_suffix(True)\n",
      "Load feature data...\tloaded!\n",
      "Load label data...\tloaded!\n"
     ]
    }
   ],
   "source": [
    "filter_size = 180\n",
    "target_size = 30\n",
    "stride = 1\n",
    "X, y = load(filter_size, target_size, stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Train/Test/VAlid 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "test_size = 0.2\n",
    "\n",
    "X_trn, X_test, y_trn, y_test = train_test_split(\n",
    "    X, y, \n",
    "    shuffle=True, \n",
    "    test_size=test_size, \n",
    "    random_state=random_state\n",
    ") \n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_trn, y_trn, \n",
    "    shuffle=True, \n",
    "    test_size=test_size, \n",
    "    random_state=random_state\n",
    ") \n",
    "\n",
    "### 스케일링 필요 시 다음을 진행(타깃에 대한 스케일링은 진행되지 않음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler()\n",
    "# X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "# X_valid_scaled = pd.DataFrame(scaler.transform(X_valid), columns=X_valid.columns)\n",
    "# X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D TO 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_to_multi(df):\n",
    "    feature_num=len(set(df.columns))\n",
    "    window_num=int(df.shape[1]/feature_num)\n",
    "    sample_num=int(df.shape[0])\n",
    "    temp=np.empty([sample_num,window_num,feature_num])\n",
    "    for i in range(feature_num):\n",
    "        temp[:,:,i]=df.iloc[:,window_num*i:window_num*i+window_num]\n",
    "    \n",
    "    return temp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>duration</th>\n",
       "      <th>duration</th>\n",
       "      <th>duration</th>\n",
       "      <th>duration</th>\n",
       "      <th>duration</th>\n",
       "      <th>duration</th>\n",
       "      <th>duration</th>\n",
       "      <th>duration</th>\n",
       "      <th>duration</th>\n",
       "      <th>...</th>\n",
       "      <th>no_upload_interval</th>\n",
       "      <th>no_upload_interval</th>\n",
       "      <th>no_upload_interval</th>\n",
       "      <th>no_upload_interval</th>\n",
       "      <th>no_upload_interval</th>\n",
       "      <th>no_upload_interval</th>\n",
       "      <th>no_upload_interval</th>\n",
       "      <th>no_upload_interval</th>\n",
       "      <th>no_upload_interval</th>\n",
       "      <th>no_upload_interval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20038</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60506</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17086</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66033</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>125</td>\n",
       "      <td>126</td>\n",
       "      <td>127</td>\n",
       "      <td>128</td>\n",
       "      <td>129</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>132</td>\n",
       "      <td>133</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16674</th>\n",
       "      <td>3.02</td>\n",
       "      <td>2.52</td>\n",
       "      <td>3.02</td>\n",
       "      <td>3.02</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.93</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24762</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31171</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>183</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>186</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>189</td>\n",
       "      <td>190</td>\n",
       "      <td>191</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56975</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31994</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30348</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50980 rows × 3060 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       duration  duration  duration  duration  duration  duration  duration  \\\n",
       "20038      0.00      0.00      0.00      0.00       0.0       0.0     13.22   \n",
       "60506      0.00      0.00      0.00      0.00       0.0       0.0      0.00   \n",
       "17086      0.00      0.00      0.00      0.00       0.0       0.0      0.00   \n",
       "66033      0.00      0.00      0.00      0.00       0.0       0.0      0.00   \n",
       "16674      3.02      2.52      3.02      3.02       3.1       0.0      3.18   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "24762      0.00      0.00      0.00      0.00       0.0       0.0      0.00   \n",
       "31171      0.00      0.00      0.00      0.00       0.0       0.0      0.00   \n",
       "56975      0.00      0.00      0.00      0.00       0.0       0.0      0.00   \n",
       "31994      0.00      0.00      0.00     13.12       0.0       0.0      0.00   \n",
       "30348      0.00      0.00      0.00      0.00       0.0       0.0      0.00   \n",
       "\n",
       "       duration  duration  duration  ...  no_upload_interval  \\\n",
       "20038      0.00      13.3      0.00  ...                   1   \n",
       "60506      0.00       0.0      0.00  ...                   0   \n",
       "17086      0.00       0.0      0.00  ...                  16   \n",
       "66033      0.00       0.0      0.00  ...                 125   \n",
       "16674      3.25       0.0      2.93  ...                   0   \n",
       "...         ...       ...       ...  ...                 ...   \n",
       "24762      0.00       0.0      0.00  ...                   0   \n",
       "31171      0.00       0.0      0.00  ...                 183   \n",
       "56975      0.00       0.0      0.00  ...                   0   \n",
       "31994      0.00       0.0      0.00  ...                   6   \n",
       "30348      0.00       0.0      0.00  ...                   0   \n",
       "\n",
       "       no_upload_interval  no_upload_interval  no_upload_interval  \\\n",
       "20038                   2                   3                   4   \n",
       "60506                   0                   0                   0   \n",
       "17086                   0                   1                   2   \n",
       "66033                 126                 127                 128   \n",
       "16674                   0                   0                   0   \n",
       "...                   ...                 ...                 ...   \n",
       "24762                   0                   1                   2   \n",
       "31171                 184                 185                 186   \n",
       "56975                   0                   0                   0   \n",
       "31994                   0                   1                   2   \n",
       "30348                   0                   0                   0   \n",
       "\n",
       "       no_upload_interval  no_upload_interval  no_upload_interval  \\\n",
       "20038                   5                   0                   1   \n",
       "60506                   0                   0                   0   \n",
       "17086                   0                   1                   2   \n",
       "66033                 129                 130                 131   \n",
       "16674                   1                   2                   0   \n",
       "...                   ...                 ...                 ...   \n",
       "24762                   3                   0                   0   \n",
       "31171                 187                 188                 189   \n",
       "56975                   0                   0                   0   \n",
       "31994                   3                   0                   1   \n",
       "30348                   0                   0                   0   \n",
       "\n",
       "       no_upload_interval  no_upload_interval  no_upload_interval  \n",
       "20038                   2                   3                   0  \n",
       "60506                   0                   0                   0  \n",
       "17086                   3                   4                   5  \n",
       "66033                 132                 133                 134  \n",
       "16674                   0                   0                   1  \n",
       "...                   ...                 ...                 ...  \n",
       "24762                   0                   1                   0  \n",
       "31171                 190                 191                 192  \n",
       "56975                   0                   0                   0  \n",
       "31994                   2                   0                   1  \n",
       "30348                   0                   0                   0  \n",
       "\n",
       "[50980 rows x 3060 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3d=stack_to_multi(X_train)\n",
    "X_valid_3d=stack_to_multi(X_valid)\n",
    "X_test_3d=stack_to_multi(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50980, 180, 17)\n",
      "(12745, 180, 17)\n",
      "(15932, 180, 17)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_3d.shape)\n",
    "print(X_valid_3d.shape)\n",
    "print(X_test_3d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50980, 30)\n",
      "(12745, 30)\n",
      "(15932, 30)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_valid.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3D 스케일링, 타겟 스케일링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=preprocessing.MinMaxScaler()\n",
    "#scaler_y=preprocessing.MinMaxScaler()\n",
    "def fit_3d(x_train,x_val,x_test):\n",
    "    x_train_sample = x_train.shape[0] #샘플 개수\n",
    "    x_val_sample=x_val.shape[0]\n",
    "    x_test_sample=x_test.shape[0]\n",
    "    \n",
    "    x_timestep = x_train.shape[1] # timestep\n",
    "    x_feature = x_train.shape[2]# feature 차원 \n",
    "    scaler=MinMaxScaler()\n",
    "    for ss in range(x_timestep):\n",
    "        scaler.partial_fit(x_train[:, ss, :]) # 순회피팅\n",
    "\n",
    "    results1,results2,results3=([],[],[])\n",
    "    for ss in range(x_timestep):\n",
    "        results1.append(scaler.transform(x_train[:, ss, :]).reshape(x_train_sample, 1, x_feature))\n",
    "        results2.append(scaler.transform(x_val[:,ss,:]).reshape(x_val_sample,1,x_feature))\n",
    "        results3.append(scaler.transform(x_test[:,ss,:]).reshape(x_test_sample,1,x_feature))\n",
    "    df_train_scaled = np.concatenate(results1, axis=1) #합치기.\n",
    "    df_val_scaled=np.concatenate(results2,axis=1)\n",
    "    df_test_scaled=np.concatenate(results3,axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return df_train_scaled,df_val_scaled,df_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled,X_valid_scaled,X_test_scaled=fit_3d(X_train_3d, X_valid_3d, X_test_3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> target shape 변경 ( 샘플 x 타입스텝 x 예측일수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_subdiff=X_train['sub_diff']\n",
    "X_valid_subdiff=X_valid['sub_diff']\n",
    "X_test_subdiff=X_test['sub_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>...</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20038</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60506</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17086</th>\n",
       "      <td>200.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66033</th>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16674</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24762</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31171</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56975</th>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31994</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30348</th>\n",
       "      <td>60.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50980 rows × 180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sub_diff  sub_diff  sub_diff  sub_diff  sub_diff  sub_diff  sub_diff  \\\n",
       "20038       1.0       1.0       1.0       1.0       1.0       1.0       1.0   \n",
       "60506      10.0      10.0      10.0       0.0      20.0      10.0      10.0   \n",
       "17086     200.0     100.0     100.0     100.0     200.0     100.0     100.0   \n",
       "66033      50.0      50.0       0.0       0.0       0.0       0.0      50.0   \n",
       "16674       1.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "24762       0.0       0.0       0.0      50.0      50.0       0.0       0.0   \n",
       "31171       0.0       0.0       0.0     100.0       0.0       0.0       0.0   \n",
       "56975       0.0      50.0      51.0     100.0     100.0     100.0     100.0   \n",
       "31994       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "30348      60.0      20.0      20.0      20.0      40.0      60.0      10.0   \n",
       "\n",
       "       sub_diff  sub_diff  sub_diff  ...  sub_diff  sub_diff  sub_diff  \\\n",
       "20038       1.0       1.0       1.0  ...       0.0       0.0      -1.0   \n",
       "60506      10.0      10.0      20.0  ...       0.0       0.0     100.0   \n",
       "17086      99.0     200.0     101.0  ...     100.0     100.0       0.0   \n",
       "66033      50.0       0.0       0.0  ...       0.0       0.0    -100.0   \n",
       "16674       0.0       0.0       0.0  ...     100.0       0.0     100.0   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "24762       0.0       0.0       0.0  ...       0.0     100.0       0.0   \n",
       "31171       0.0       0.0       0.0  ...       0.0       0.0       0.0   \n",
       "56975     100.0     300.0       0.0  ...     100.0     100.0     100.0   \n",
       "31994       0.0       0.0       0.0  ...       0.0     100.0     700.0   \n",
       "30348      10.0      25.0      25.0  ...       0.0       0.0       0.0   \n",
       "\n",
       "       sub_diff  sub_diff  sub_diff  sub_diff  sub_diff  sub_diff  sub_diff  \n",
       "20038       0.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "60506       0.0       0.0     100.0      99.0     101.0     100.0       0.0  \n",
       "17086      99.0     101.0     100.0       0.0     100.0       0.0      99.0  \n",
       "66033       0.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "16674      50.0      50.0       0.0     100.0      50.0      50.0     100.0  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "24762       0.0     100.0     100.0       0.0       0.0     100.0       0.0  \n",
       "31171       0.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "56975       0.0       0.0     100.0     100.0     200.0     100.0     100.0  \n",
       "31994     300.0     300.0     200.0     100.0     200.0     100.0      99.0  \n",
       "30348       0.0     100.0       0.0       0.0       0.0       0.0       0.0  \n",
       "\n",
       "[50980 rows x 180 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_subdiff[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (샘플개수 , (180+30)의 TARGET\n",
    "temp1=pd.concat([X_train_subdiff,y_train],axis=1)\n",
    "temp2=pd.concat([X_valid_subdiff,y_valid],axis=1)\n",
    "temp3=pd.concat([X_test_subdiff,y_test],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (샘플, timestep, targetsize)\n",
    "y_train_3d=np.empty((X_train_subdiff.shape[0],X_train_subdiff.shape[1],30)) # 30 : targetsize\n",
    "y_valid_3d=np.empty((X_valid_subdiff.shape[0],X_valid_subdiff.shape[1],30))\n",
    "y_test_3d=np.empty((X_test_subdiff.shape[0],X_test_subdiff.shape[1],30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(X_train_subdiff.shape[1]): # 180 Timestep\n",
    "    y_train_3d[:,t]=temp1.iloc[:,t:t+30]\n",
    "    y_valid_3d[:,t]=temp2.iloc[:,t:t+30]\n",
    "    y_test_3d[:,t]=temp3.iloc[:,t:t+30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50980, 180, 30)\n",
      "(12745, 180, 30)\n",
      "(15932, 180, 30)\n"
     ]
    }
   ],
   "source": [
    "print(y_train_3d.shape)\n",
    "print(y_valid_3d.shape)\n",
    "print(y_test_3d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> y scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_temp=y_train_3d.reshape(-1,1)\n",
    "y_valid_temp=y_valid_3d.reshape(-1,1)\n",
    "y_test_temp=y_test_3d.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_y=MinMaxScaler()\n",
    "y_train_scaled=scaler_y.fit_transform(y_train_temp)\n",
    "y_val_scaled=scaler_y.transform(y_valid_temp)\n",
    "y_test_scaled=scaler_y.transform(y_test_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_scaled=y_train_scaled.reshape(y_train_3d.shape[0],y_train_3d.shape[1],y_train_3d.shape[2])\n",
    "y_valid_scaled=y_val_scaled.reshape(y_valid_3d.shape[0],y_valid_3d.shape[1],y_valid_3d.shape[2])\n",
    "y_test_scaled=y_test_scaled.reshape(y_test_3d.shape[0],y_test_3d.shape[1],y_test_3d.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50980, 180, 30)\n",
      "(12745, 180, 30)\n",
      "(15932, 180, 30)\n"
     ]
    }
   ],
   "source": [
    "print(y_train_scaled.shape)\n",
    "print(y_valid_scaled.shape)\n",
    "print(y_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 마지막 Timestep의 loss만 중요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_time_step_mse(Y_true, Y_pred):\n",
    "    return keras.metrics.mean_absolute_error(Y_true[:, -1], Y_pred[:, -1]) #### <<- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wavenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='ConvGRU(180_30_1).h5'\n",
    "callback_list1 = [tf.keras.callbacks.ModelCheckpoint(filepath='Checkpoint/{}'.format(filename),\n",
    "                                                    monitor='val_last_time_step_mse',\n",
    "                                                    verbose=1,\n",
    "                                                    save_best_only=True,\n",
    "                                                    mode='min'),\n",
    "                 tf.keras.callbacks.EarlyStopping(monitor='val_last_time_step_mse',\n",
    "                                                  patience=15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, None, 180)         15480     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, None, 180)         195480    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, None, 180)         195480    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 30)          5430      \n",
      "=================================================================\n",
      "Total params: 411,870\n",
      "Trainable params: 411,870\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model= keras.models.Sequential([\n",
    "    keras.layers.Conv1D(filters=180, kernel_size=5, strides=3, padding=\"valid\",\n",
    "                        input_shape=[None, 17]),\n",
    "    #-------뭐라도 해보기 ----- OK, !! \n",
    "    #keras.layers.MaxPooling1D(pool_size=1, strides=None),\n",
    "    keras.layers.GRU(180, return_sequences=True),\n",
    "    keras.layers.GRU(180, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(30))\n",
    "    ,\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mae\", optimizer=\"adam\", metrics=[last_time_step_mse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 0.0023 - last_time_step_mse: 0.0024\n",
      "Epoch 00001: val_last_time_step_mse improved from inf to 0.00119, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 256s 161ms/step - loss: 0.0023 - last_time_step_mse: 0.0024 - val_loss: 0.0011 - val_last_time_step_mse: 0.0012\n",
      "Epoch 2/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 0.0011 - last_time_step_mse: 0.0012\n",
      "Epoch 00002: val_last_time_step_mse did not improve from 0.00119\n",
      "1594/1594 [==============================] - 238s 149ms/step - loss: 0.0011 - last_time_step_mse: 0.0012 - val_loss: 0.0012 - val_last_time_step_mse: 0.0012\n",
      "Epoch 3/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 0.0011 - last_time_step_mse: 0.0012\n",
      "Epoch 00003: val_last_time_step_mse improved from 0.00119 to 0.00110, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 241s 151ms/step - loss: 0.0011 - last_time_step_mse: 0.0012 - val_loss: 0.0010 - val_last_time_step_mse: 0.0011\n",
      "Epoch 4/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 9.8935e-04 - last_time_step_mse: 0.0011\n",
      "Epoch 00004: val_last_time_step_mse did not improve from 0.00110\n",
      "1594/1594 [==============================] - 251s 158ms/step - loss: 9.8935e-04 - last_time_step_mse: 0.0011 - val_loss: 0.0011 - val_last_time_step_mse: 0.0011\n",
      "Epoch 5/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 9.3897e-04 - last_time_step_mse: 0.0010\n",
      "Epoch 00005: val_last_time_step_mse improved from 0.00110 to 0.00104, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 251s 157ms/step - loss: 9.3897e-04 - last_time_step_mse: 0.0010 - val_loss: 9.5803e-04 - val_last_time_step_mse: 0.0010\n",
      "Epoch 6/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 9.1156e-04 - last_time_step_mse: 9.9466e-04\n",
      "Epoch 00006: val_last_time_step_mse improved from 0.00104 to 0.00095, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 231s 145ms/step - loss: 9.1156e-04 - last_time_step_mse: 9.9466e-04 - val_loss: 8.9247e-04 - val_last_time_step_mse: 9.4605e-04\n",
      "Epoch 7/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 8.7914e-04 - last_time_step_mse: 9.5795e-04\n",
      "Epoch 00007: val_last_time_step_mse improved from 0.00095 to 0.00094, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 236s 148ms/step - loss: 8.7914e-04 - last_time_step_mse: 9.5795e-04 - val_loss: 8.5588e-04 - val_last_time_step_mse: 9.3925e-04\n",
      "Epoch 8/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 8.5583e-04 - last_time_step_mse: 9.3561e-04\n",
      "Epoch 00008: val_last_time_step_mse improved from 0.00094 to 0.00092, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 238s 150ms/step - loss: 8.5583e-04 - last_time_step_mse: 9.3561e-04 - val_loss: 8.4231e-04 - val_last_time_step_mse: 9.1911e-04\n",
      "Epoch 9/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 8.3863e-04 - last_time_step_mse: 9.1873e-04\n",
      "Epoch 00009: val_last_time_step_mse did not improve from 0.00092\n",
      "1594/1594 [==============================] - 241s 151ms/step - loss: 8.3863e-04 - last_time_step_mse: 9.1873e-04 - val_loss: 9.2265e-04 - val_last_time_step_mse: 9.9905e-04\n",
      "Epoch 10/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 8.2910e-04 - last_time_step_mse: 9.0925e-04\n",
      "Epoch 00010: val_last_time_step_mse did not improve from 0.00092\n",
      "1594/1594 [==============================] - 234s 147ms/step - loss: 8.2910e-04 - last_time_step_mse: 9.0925e-04 - val_loss: 9.0029e-04 - val_last_time_step_mse: 9.7577e-04\n",
      "Epoch 11/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 8.1872e-04 - last_time_step_mse: 8.9771e-04\n",
      "Epoch 00011: val_last_time_step_mse did not improve from 0.00092\n",
      "1594/1594 [==============================] - 238s 150ms/step - loss: 8.1872e-04 - last_time_step_mse: 8.9771e-04 - val_loss: 9.9437e-04 - val_last_time_step_mse: 0.0010\n",
      "Epoch 12/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 8.0217e-04 - last_time_step_mse: 8.8028e-04\n",
      "Epoch 00012: val_last_time_step_mse did not improve from 0.00092\n",
      "1594/1594 [==============================] - 238s 149ms/step - loss: 8.0217e-04 - last_time_step_mse: 8.8028e-04 - val_loss: 9.1166e-04 - val_last_time_step_mse: 9.3337e-04\n",
      "Epoch 13/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 7.9804e-04 - last_time_step_mse: 8.7507e-04\n",
      "Epoch 00013: val_last_time_step_mse improved from 0.00092 to 0.00085, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 241s 151ms/step - loss: 7.9804e-04 - last_time_step_mse: 8.7507e-04 - val_loss: 7.8443e-04 - val_last_time_step_mse: 8.4850e-04\n",
      "Epoch 14/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 7.9264e-04 - last_time_step_mse: 8.6742e-04\n",
      "Epoch 00014: val_last_time_step_mse improved from 0.00085 to 0.00082, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 229s 144ms/step - loss: 7.9264e-04 - last_time_step_mse: 8.6742e-04 - val_loss: 7.6174e-04 - val_last_time_step_mse: 8.1808e-04\n",
      "Epoch 15/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 7.6427e-04 - last_time_step_mse: 8.3753e-04\n",
      "Epoch 00015: val_last_time_step_mse did not improve from 0.00082\n",
      "1594/1594 [==============================] - 234s 147ms/step - loss: 7.6427e-04 - last_time_step_mse: 8.3753e-04 - val_loss: 7.8034e-04 - val_last_time_step_mse: 8.4980e-04\n",
      "Epoch 16/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 7.5840e-04 - last_time_step_mse: 8.3035e-04\n",
      "Epoch 00016: val_last_time_step_mse did not improve from 0.00082\n",
      "1594/1594 [==============================] - 230s 144ms/step - loss: 7.5840e-04 - last_time_step_mse: 8.3035e-04 - val_loss: 7.5915e-04 - val_last_time_step_mse: 8.2437e-04\n",
      "Epoch 17/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 7.4034e-04 - last_time_step_mse: 8.1045e-04\n",
      "Epoch 00017: val_last_time_step_mse did not improve from 0.00082\n",
      "1594/1594 [==============================] - 227s 142ms/step - loss: 7.4034e-04 - last_time_step_mse: 8.1045e-04 - val_loss: 8.0362e-04 - val_last_time_step_mse: 8.6685e-04\n",
      "Epoch 18/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 7.2665e-04 - last_time_step_mse: 7.9362e-04\n",
      "Epoch 00018: val_last_time_step_mse improved from 0.00082 to 0.00076, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 229s 143ms/step - loss: 7.2665e-04 - last_time_step_mse: 7.9362e-04 - val_loss: 7.0783e-04 - val_last_time_step_mse: 7.6040e-04\n",
      "Epoch 19/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 7.1074e-04 - last_time_step_mse: 7.7738e-04\n",
      "Epoch 00019: val_last_time_step_mse improved from 0.00076 to 0.00075, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 223s 140ms/step - loss: 7.1074e-04 - last_time_step_mse: 7.7738e-04 - val_loss: 7.0196e-04 - val_last_time_step_mse: 7.4977e-04\n",
      "Epoch 20/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 7.0242e-04 - last_time_step_mse: 7.6511e-04\n",
      "Epoch 00020: val_last_time_step_mse did not improve from 0.00075\n",
      "1594/1594 [==============================] - 230s 144ms/step - loss: 7.0242e-04 - last_time_step_mse: 7.6511e-04 - val_loss: 7.1651e-04 - val_last_time_step_mse: 7.6460e-04\n",
      "Epoch 21/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 6.9096e-04 - last_time_step_mse: 7.5374e-04\n",
      "Epoch 00021: val_last_time_step_mse did not improve from 0.00075\n",
      "1594/1594 [==============================] - 226s 142ms/step - loss: 6.9096e-04 - last_time_step_mse: 7.5374e-04 - val_loss: 7.0393e-04 - val_last_time_step_mse: 7.5651e-04\n",
      "Epoch 22/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 6.8600e-04 - last_time_step_mse: 7.4780e-04\n",
      "Epoch 00022: val_last_time_step_mse improved from 0.00075 to 0.00073, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 226s 142ms/step - loss: 6.8600e-04 - last_time_step_mse: 7.4780e-04 - val_loss: 6.8578e-04 - val_last_time_step_mse: 7.3292e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 6.7698e-04 - last_time_step_mse: 7.3779e-04\n",
      "Epoch 00023: val_last_time_step_mse did not improve from 0.00073\n",
      "1594/1594 [==============================] - 230s 144ms/step - loss: 6.7698e-04 - last_time_step_mse: 7.3779e-04 - val_loss: 7.0024e-04 - val_last_time_step_mse: 7.4012e-04\n",
      "Epoch 24/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 6.6876e-04 - last_time_step_mse: 7.2797e-04\n",
      "Epoch 00024: val_last_time_step_mse improved from 0.00073 to 0.00073, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 223s 140ms/step - loss: 6.6876e-04 - last_time_step_mse: 7.2797e-04 - val_loss: 6.8350e-04 - val_last_time_step_mse: 7.2503e-04\n",
      "Epoch 25/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 6.6183e-04 - last_time_step_mse: 7.1767e-04\n",
      "Epoch 00025: val_last_time_step_mse improved from 0.00073 to 0.00070, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 229s 144ms/step - loss: 6.6183e-04 - last_time_step_mse: 7.1767e-04 - val_loss: 6.5292e-04 - val_last_time_step_mse: 6.9776e-04\n",
      "Epoch 26/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 6.5447e-04 - last_time_step_mse: 7.0862e-04\n",
      "Epoch 00026: val_last_time_step_mse did not improve from 0.00070\n",
      "1594/1594 [==============================] - 229s 143ms/step - loss: 6.5447e-04 - last_time_step_mse: 7.0862e-04 - val_loss: 6.8445e-04 - val_last_time_step_mse: 7.2903e-04\n",
      "Epoch 27/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 6.4676e-04 - last_time_step_mse: 6.9922e-04\n",
      "Epoch 00027: val_last_time_step_mse improved from 0.00070 to 0.00069, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 221s 139ms/step - loss: 6.4676e-04 - last_time_step_mse: 6.9922e-04 - val_loss: 6.4328e-04 - val_last_time_step_mse: 6.9021e-04\n",
      "Epoch 28/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 6.3661e-04 - last_time_step_mse: 6.8692e-04\n",
      "Epoch 00028: val_last_time_step_mse did not improve from 0.00069\n",
      "1594/1594 [==============================] - 226s 142ms/step - loss: 6.3661e-04 - last_time_step_mse: 6.8692e-04 - val_loss: 7.5096e-04 - val_last_time_step_mse: 7.7887e-04\n",
      "Epoch 29/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 6.3002e-04 - last_time_step_mse: 6.7687e-04\n",
      "Epoch 00029: val_last_time_step_mse improved from 0.00069 to 0.00068, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 229s 144ms/step - loss: 6.3002e-04 - last_time_step_mse: 6.7687e-04 - val_loss: 6.4727e-04 - val_last_time_step_mse: 6.7818e-04\n",
      "Epoch 30/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 6.2335e-04 - last_time_step_mse: 6.6884e-04\n",
      "Epoch 00030: val_last_time_step_mse improved from 0.00068 to 0.00065, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 227s 142ms/step - loss: 6.2335e-04 - last_time_step_mse: 6.6884e-04 - val_loss: 6.1657e-04 - val_last_time_step_mse: 6.4944e-04\n",
      "Epoch 31/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 6.1617e-04 - last_time_step_mse: 6.5851e-04\n",
      "Epoch 00031: val_last_time_step_mse improved from 0.00065 to 0.00064, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 226s 142ms/step - loss: 6.1617e-04 - last_time_step_mse: 6.5851e-04 - val_loss: 6.0500e-04 - val_last_time_step_mse: 6.3999e-04\n",
      "Epoch 32/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 6.0978e-04 - last_time_step_mse: 6.4957e-04\n",
      "Epoch 00032: val_last_time_step_mse did not improve from 0.00064\n",
      "1594/1594 [==============================] - 222s 140ms/step - loss: 6.0978e-04 - last_time_step_mse: 6.4957e-04 - val_loss: 6.3995e-04 - val_last_time_step_mse: 6.7451e-04\n",
      "Epoch 33/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 6.0291e-04 - last_time_step_mse: 6.4113e-04\n",
      "Epoch 00033: val_last_time_step_mse improved from 0.00064 to 0.00063, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 225s 141ms/step - loss: 6.0291e-04 - last_time_step_mse: 6.4113e-04 - val_loss: 5.9889e-04 - val_last_time_step_mse: 6.2855e-04\n",
      "Epoch 34/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 6.0017e-04 - last_time_step_mse: 6.3625e-04\n",
      "Epoch 00034: val_last_time_step_mse improved from 0.00063 to 0.00063, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 231s 145ms/step - loss: 6.0017e-04 - last_time_step_mse: 6.3625e-04 - val_loss: 5.9912e-04 - val_last_time_step_mse: 6.2690e-04\n",
      "Epoch 35/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.9237e-04 - last_time_step_mse: 6.2567e-04\n",
      "Epoch 00035: val_last_time_step_mse improved from 0.00063 to 0.00062, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 221s 139ms/step - loss: 5.9237e-04 - last_time_step_mse: 6.2567e-04 - val_loss: 6.1970e-04 - val_last_time_step_mse: 6.2424e-04\n",
      "Epoch 36/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.8756e-04 - last_time_step_mse: 6.1914e-04\n",
      "Epoch 00036: val_last_time_step_mse improved from 0.00062 to 0.00061, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 232s 145ms/step - loss: 5.8756e-04 - last_time_step_mse: 6.1914e-04 - val_loss: 5.9611e-04 - val_last_time_step_mse: 6.1231e-04\n",
      "Epoch 37/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.8561e-04 - last_time_step_mse: 6.1997e-04\n",
      "Epoch 00037: val_last_time_step_mse improved from 0.00061 to 0.00061, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 223s 140ms/step - loss: 5.8561e-04 - last_time_step_mse: 6.1997e-04 - val_loss: 5.8813e-04 - val_last_time_step_mse: 6.0532e-04\n",
      "Epoch 38/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.7792e-04 - last_time_step_mse: 6.0896e-04\n",
      "Epoch 00038: val_last_time_step_mse did not improve from 0.00061\n",
      "1594/1594 [==============================] - 225s 141ms/step - loss: 5.7792e-04 - last_time_step_mse: 6.0896e-04 - val_loss: 5.9049e-04 - val_last_time_step_mse: 6.0927e-04\n",
      "Epoch 39/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.7262e-04 - last_time_step_mse: 6.0211e-04\n",
      "Epoch 00039: val_last_time_step_mse improved from 0.00061 to 0.00060, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 221s 139ms/step - loss: 5.7262e-04 - last_time_step_mse: 6.0211e-04 - val_loss: 5.7792e-04 - val_last_time_step_mse: 6.0115e-04\n",
      "Epoch 40/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.6867e-04 - last_time_step_mse: 5.9673e-04\n",
      "Epoch 00040: val_last_time_step_mse improved from 0.00060 to 0.00059, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 227s 143ms/step - loss: 5.6867e-04 - last_time_step_mse: 5.9673e-04 - val_loss: 5.6706e-04 - val_last_time_step_mse: 5.9191e-04\n",
      "Epoch 41/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.6355e-04 - last_time_step_mse: 5.9129e-04\n",
      "Epoch 00041: val_last_time_step_mse improved from 0.00059 to 0.00058, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 219s 138ms/step - loss: 5.6355e-04 - last_time_step_mse: 5.9129e-04 - val_loss: 5.6432e-04 - val_last_time_step_mse: 5.8408e-04\n",
      "Epoch 42/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.6028e-04 - last_time_step_mse: 5.8659e-04\n",
      "Epoch 00042: val_last_time_step_mse improved from 0.00058 to 0.00058, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 218s 137ms/step - loss: 5.6028e-04 - last_time_step_mse: 5.8659e-04 - val_loss: 5.6215e-04 - val_last_time_step_mse: 5.7796e-04\n",
      "Epoch 43/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.6107e-04 - last_time_step_mse: 5.8738e-04\n",
      "Epoch 00043: val_last_time_step_mse did not improve from 0.00058\n",
      "1594/1594 [==============================] - 226s 142ms/step - loss: 5.6107e-04 - last_time_step_mse: 5.8738e-04 - val_loss: 5.8861e-04 - val_last_time_step_mse: 6.0233e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.5408e-04 - last_time_step_mse: 5.7860e-04\n",
      "Epoch 00044: val_last_time_step_mse improved from 0.00058 to 0.00056, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 198s 124ms/step - loss: 5.5408e-04 - last_time_step_mse: 5.7860e-04 - val_loss: 5.4878e-04 - val_last_time_step_mse: 5.6460e-04\n",
      "Epoch 45/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.4876e-04 - last_time_step_mse: 5.7050e-04\n",
      "Epoch 00045: val_last_time_step_mse did not improve from 0.00056\n",
      "1594/1594 [==============================] - 193s 121ms/step - loss: 5.4876e-04 - last_time_step_mse: 5.7050e-04 - val_loss: 5.8976e-04 - val_last_time_step_mse: 5.9103e-04\n",
      "Epoch 46/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.4743e-04 - last_time_step_mse: 5.6959e-04\n",
      "Epoch 00046: val_last_time_step_mse improved from 0.00056 to 0.00056, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 193s 121ms/step - loss: 5.4743e-04 - last_time_step_mse: 5.6959e-04 - val_loss: 5.6396e-04 - val_last_time_step_mse: 5.6374e-04\n",
      "Epoch 47/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.4349e-04 - last_time_step_mse: 5.6362e-04\n",
      "Epoch 00047: val_last_time_step_mse improved from 0.00056 to 0.00055, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 195s 122ms/step - loss: 5.4349e-04 - last_time_step_mse: 5.6362e-04 - val_loss: 5.4201e-04 - val_last_time_step_mse: 5.5041e-04\n",
      "Epoch 48/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.4007e-04 - last_time_step_mse: 5.5988e-04\n",
      "Epoch 00048: val_last_time_step_mse improved from 0.00055 to 0.00055, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 193s 121ms/step - loss: 5.4007e-04 - last_time_step_mse: 5.5988e-04 - val_loss: 5.3693e-04 - val_last_time_step_mse: 5.4981e-04\n",
      "Epoch 49/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.3826e-04 - last_time_step_mse: 5.5776e-04\n",
      "Epoch 00049: val_last_time_step_mse did not improve from 0.00055\n",
      "1594/1594 [==============================] - 194s 122ms/step - loss: 5.3826e-04 - last_time_step_mse: 5.5776e-04 - val_loss: 5.6839e-04 - val_last_time_step_mse: 5.8670e-04\n",
      "Epoch 50/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.3457e-04 - last_time_step_mse: 5.5245e-04\n",
      "Epoch 00050: val_last_time_step_mse improved from 0.00055 to 0.00055, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 194s 122ms/step - loss: 5.3457e-04 - last_time_step_mse: 5.5245e-04 - val_loss: 5.3538e-04 - val_last_time_step_mse: 5.4760e-04\n",
      "Epoch 51/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.3236e-04 - last_time_step_mse: 5.4986e-04\n",
      "Epoch 00051: val_last_time_step_mse did not improve from 0.00055\n",
      "1594/1594 [==============================] - 193s 121ms/step - loss: 5.3236e-04 - last_time_step_mse: 5.4986e-04 - val_loss: 5.3845e-04 - val_last_time_step_mse: 5.5032e-04\n",
      "Epoch 52/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.3050e-04 - last_time_step_mse: 5.4790e-04\n",
      "Epoch 00052: val_last_time_step_mse improved from 0.00055 to 0.00054, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 191s 120ms/step - loss: 5.3050e-04 - last_time_step_mse: 5.4790e-04 - val_loss: 5.3523e-04 - val_last_time_step_mse: 5.4227e-04\n",
      "Epoch 53/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.2789e-04 - last_time_step_mse: 5.4516e-04\n",
      "Epoch 00053: val_last_time_step_mse did not improve from 0.00054\n",
      "1594/1594 [==============================] - 193s 121ms/step - loss: 5.2789e-04 - last_time_step_mse: 5.4516e-04 - val_loss: 5.5954e-04 - val_last_time_step_mse: 5.5721e-04\n",
      "Epoch 54/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.2552e-04 - last_time_step_mse: 5.4034e-04\n",
      "Epoch 00054: val_last_time_step_mse improved from 0.00054 to 0.00054, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 193s 121ms/step - loss: 5.2552e-04 - last_time_step_mse: 5.4034e-04 - val_loss: 5.3914e-04 - val_last_time_step_mse: 5.3909e-04\n",
      "Epoch 55/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.2464e-04 - last_time_step_mse: 5.4107e-04\n",
      "Epoch 00055: val_last_time_step_mse improved from 0.00054 to 0.00053, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 189s 119ms/step - loss: 5.2464e-04 - last_time_step_mse: 5.4107e-04 - val_loss: 5.2242e-04 - val_last_time_step_mse: 5.2758e-04\n",
      "Epoch 56/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.2062e-04 - last_time_step_mse: 5.3497e-04\n",
      "Epoch 00056: val_last_time_step_mse did not improve from 0.00053\n",
      "1594/1594 [==============================] - 192s 120ms/step - loss: 5.2062e-04 - last_time_step_mse: 5.3497e-04 - val_loss: 5.2656e-04 - val_last_time_step_mse: 5.3266e-04\n",
      "Epoch 57/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.1895e-04 - last_time_step_mse: 5.3345e-04\n",
      "Epoch 00057: val_last_time_step_mse did not improve from 0.00053\n",
      "1594/1594 [==============================] - 189s 118ms/step - loss: 5.1895e-04 - last_time_step_mse: 5.3345e-04 - val_loss: 5.2603e-04 - val_last_time_step_mse: 5.3762e-04\n",
      "Epoch 58/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.1776e-04 - last_time_step_mse: 5.3235e-04\n",
      "Epoch 00058: val_last_time_step_mse did not improve from 0.00053\n",
      "1594/1594 [==============================] - 190s 119ms/step - loss: 5.1776e-04 - last_time_step_mse: 5.3235e-04 - val_loss: 5.4283e-04 - val_last_time_step_mse: 5.2976e-04\n",
      "Epoch 59/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.1452e-04 - last_time_step_mse: 5.2726e-04\n",
      "Epoch 00059: val_last_time_step_mse improved from 0.00053 to 0.00052, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 191s 120ms/step - loss: 5.1452e-04 - last_time_step_mse: 5.2726e-04 - val_loss: 5.1451e-04 - val_last_time_step_mse: 5.1958e-04\n",
      "Epoch 60/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.1287e-04 - last_time_step_mse: 5.2612e-04\n",
      "Epoch 00060: val_last_time_step_mse improved from 0.00052 to 0.00051, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 191s 120ms/step - loss: 5.1287e-04 - last_time_step_mse: 5.2612e-04 - val_loss: 5.1345e-04 - val_last_time_step_mse: 5.1149e-04\n",
      "Epoch 61/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.1217e-04 - last_time_step_mse: 5.2552e-04\n",
      "Epoch 00061: val_last_time_step_mse did not improve from 0.00051\n",
      "1594/1594 [==============================] - 191s 120ms/step - loss: 5.1217e-04 - last_time_step_mse: 5.2552e-04 - val_loss: 5.3329e-04 - val_last_time_step_mse: 5.3687e-04\n",
      "Epoch 62/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.0892e-04 - last_time_step_mse: 5.2210e-04\n",
      "Epoch 00062: val_last_time_step_mse improved from 0.00051 to 0.00050, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 190s 119ms/step - loss: 5.0892e-04 - last_time_step_mse: 5.2210e-04 - val_loss: 5.0096e-04 - val_last_time_step_mse: 5.0083e-04\n",
      "Epoch 63/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.0691e-04 - last_time_step_mse: 5.1889e-04\n",
      "Epoch 00063: val_last_time_step_mse did not improve from 0.00050\n",
      "1594/1594 [==============================] - 189s 119ms/step - loss: 5.0691e-04 - last_time_step_mse: 5.1889e-04 - val_loss: 5.1947e-04 - val_last_time_step_mse: 5.2466e-04\n",
      "Epoch 64/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.0475e-04 - last_time_step_mse: 5.1629e-04\n",
      "Epoch 00064: val_last_time_step_mse did not improve from 0.00050\n",
      "1594/1594 [==============================] - 187s 117ms/step - loss: 5.0475e-04 - last_time_step_mse: 5.1629e-04 - val_loss: 5.1536e-04 - val_last_time_step_mse: 5.1344e-04\n",
      "Epoch 65/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1594/1594 [==============================] - ETA: 0s - loss: 5.0449e-04 - last_time_step_mse: 5.1627e-04\n",
      "Epoch 00065: val_last_time_step_mse did not improve from 0.00050\n",
      "1594/1594 [==============================] - 193s 121ms/step - loss: 5.0449e-04 - last_time_step_mse: 5.1627e-04 - val_loss: 5.1108e-04 - val_last_time_step_mse: 5.1247e-04\n",
      "Epoch 66/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.0209e-04 - last_time_step_mse: 5.1480e-04\n",
      "Epoch 00066: val_last_time_step_mse improved from 0.00050 to 0.00050, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 188s 118ms/step - loss: 5.0209e-04 - last_time_step_mse: 5.1480e-04 - val_loss: 5.0200e-04 - val_last_time_step_mse: 5.0054e-04\n",
      "Epoch 67/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 5.0142e-04 - last_time_step_mse: 5.1372e-04\n",
      "Epoch 00067: val_last_time_step_mse improved from 0.00050 to 0.00049, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 188s 118ms/step - loss: 5.0142e-04 - last_time_step_mse: 5.1372e-04 - val_loss: 4.9695e-04 - val_last_time_step_mse: 4.9332e-04\n",
      "Epoch 68/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 4.9843e-04 - last_time_step_mse: 5.0907e-04\n",
      "Epoch 00068: val_last_time_step_mse did not improve from 0.00049\n",
      "1594/1594 [==============================] - 191s 120ms/step - loss: 4.9843e-04 - last_time_step_mse: 5.0907e-04 - val_loss: 5.0597e-04 - val_last_time_step_mse: 5.0531e-04\n",
      "Epoch 69/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 4.9901e-04 - last_time_step_mse: 5.0951e-04\n",
      "Epoch 00069: val_last_time_step_mse improved from 0.00049 to 0.00049, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 188s 118ms/step - loss: 4.9901e-04 - last_time_step_mse: 5.0951e-04 - val_loss: 4.9533e-04 - val_last_time_step_mse: 4.8964e-04\n",
      "Epoch 70/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 4.9680e-04 - last_time_step_mse: 5.0761e-04\n",
      "Epoch 00070: val_last_time_step_mse did not improve from 0.00049\n",
      "1594/1594 [==============================] - 190s 119ms/step - loss: 4.9680e-04 - last_time_step_mse: 5.0761e-04 - val_loss: 5.1676e-04 - val_last_time_step_mse: 5.0847e-04\n",
      "Epoch 71/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 4.9503e-04 - last_time_step_mse: 5.0430e-04\n",
      "Epoch 00071: val_last_time_step_mse improved from 0.00049 to 0.00049, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 189s 118ms/step - loss: 4.9503e-04 - last_time_step_mse: 5.0430e-04 - val_loss: 4.9831e-04 - val_last_time_step_mse: 4.8911e-04\n",
      "Epoch 72/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 4.9309e-04 - last_time_step_mse: 5.0227e-04\n",
      "Epoch 00072: val_last_time_step_mse improved from 0.00049 to 0.00049, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 191s 120ms/step - loss: 4.9309e-04 - last_time_step_mse: 5.0227e-04 - val_loss: 4.9012e-04 - val_last_time_step_mse: 4.8669e-04\n",
      "Epoch 73/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 4.9208e-04 - last_time_step_mse: 5.0223e-04\n",
      "Epoch 00073: val_last_time_step_mse did not improve from 0.00049\n",
      "1594/1594 [==============================] - 191s 120ms/step - loss: 4.9208e-04 - last_time_step_mse: 5.0223e-04 - val_loss: 4.9075e-04 - val_last_time_step_mse: 4.8670e-04\n",
      "Epoch 74/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 4.9023e-04 - last_time_step_mse: 4.9881e-04\n",
      "Epoch 00074: val_last_time_step_mse did not improve from 0.00049\n",
      "1594/1594 [==============================] - 191s 120ms/step - loss: 4.9023e-04 - last_time_step_mse: 4.9881e-04 - val_loss: 5.0122e-04 - val_last_time_step_mse: 4.9636e-04\n",
      "Epoch 75/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 4.8957e-04 - last_time_step_mse: 4.9886e-04\n",
      "Epoch 00075: val_last_time_step_mse did not improve from 0.00049\n",
      "1594/1594 [==============================] - 191s 120ms/step - loss: 4.8957e-04 - last_time_step_mse: 4.9886e-04 - val_loss: 5.0217e-04 - val_last_time_step_mse: 5.0206e-04\n",
      "Epoch 76/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 4.8858e-04 - last_time_step_mse: 4.9737e-04\n",
      "Epoch 00076: val_last_time_step_mse did not improve from 0.00049\n",
      "1594/1594 [==============================] - 194s 122ms/step - loss: 4.8858e-04 - last_time_step_mse: 4.9737e-04 - val_loss: 4.9283e-04 - val_last_time_step_mse: 4.8834e-04\n",
      "Epoch 77/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 4.8728e-04 - last_time_step_mse: 4.9457e-04\n",
      "Epoch 00077: val_last_time_step_mse did not improve from 0.00049\n",
      "1594/1594 [==============================] - 191s 120ms/step - loss: 4.8728e-04 - last_time_step_mse: 4.9457e-04 - val_loss: 4.9216e-04 - val_last_time_step_mse: 4.8810e-04\n",
      "Epoch 78/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 4.8564e-04 - last_time_step_mse: 4.9188e-04\n",
      "Epoch 00078: val_last_time_step_mse improved from 0.00049 to 0.00048, saving model to Checkpoint/ConvGRU(180_30_1).h5\n",
      "1594/1594 [==============================] - 189s 118ms/step - loss: 4.8564e-04 - last_time_step_mse: 4.9188e-04 - val_loss: 4.8385e-04 - val_last_time_step_mse: 4.7606e-04\n",
      "Epoch 79/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 4.8365e-04 - last_time_step_mse: 4.8991e-04\n",
      "Epoch 00079: val_last_time_step_mse did not improve from 0.00048\n",
      "1594/1594 [==============================] - 190s 119ms/step - loss: 4.8365e-04 - last_time_step_mse: 4.8991e-04 - val_loss: 4.9153e-04 - val_last_time_step_mse: 4.8455e-04\n",
      "Epoch 80/80\n",
      "1594/1594 [==============================] - ETA: 0s - loss: 4.8246e-04 - last_time_step_mse: 4.8773e-04\n",
      "Epoch 00080: val_last_time_step_mse did not improve from 0.00048\n",
      "1594/1594 [==============================] - 191s 120ms/step - loss: 4.8246e-04 - last_time_step_mse: 4.8773e-04 - val_loss: 4.8346e-04 - val_last_time_step_mse: 4.7665e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train_scaled[:, 4::3], epochs=80,\n",
    "                    validation_data=(X_valid_scaled,y_valid_scaled[:, 4::3]),callbacks=callback_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_Full.history['last_time_step_mse'], label='train_loss')\n",
    "plt.plot(history_Full.history['val_last_time_step_mse'], label='val_loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cancer",
   "language": "python",
   "name": "cancer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
