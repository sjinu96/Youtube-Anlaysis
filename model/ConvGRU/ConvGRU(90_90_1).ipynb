{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T03:17:33.191705Z",
     "start_time": "2020-11-20T03:17:33.186707Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, date\n",
    "from sklearn import utils\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as fn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def _fit_transform(self, raw):\n",
    "        result = raw.copy()\n",
    "\n",
    "        result = self._n_comment_to_float(result)\n",
    "        result = self._str_to_datetype(result)\n",
    "        result = self._add_n_hashtag(result)\n",
    "        \n",
    "        self.non_numeric = ['channel', 'title', 'genre', 'description', 'date', 'sign_in']\n",
    "        result = self._merge(result, self.non_numeric)\n",
    "        \n",
    "        features = ['cumul_view', 'n_dislike', 'n_like', 'n_comment', 'video_n_view', 'cumul_subs']\n",
    "        new_name = ['view_diff', 'dislike_diff', 'like_diff', 'comment_diff', 'video_n_view_diff', 'sub_diff']\n",
    "        result = self._add_diff(result, features, new_name)\n",
    "        \n",
    "        result = self._add_no_upload_interval(result)\n",
    "        result = self._remove_nan(result)\n",
    "        self._one_hot(result)\n",
    "\n",
    "        return result\n",
    "        \n",
    "        \n",
    "        \n",
    "    #FEATRUES TO ADD & MODIFY\n",
    "    ####################################################################     \n",
    "    def _n_comment_to_float(self,result):\n",
    "        idx1 = result['n_comment'] == '댓글 사용 중지'\n",
    "        idx2 = result.n_comment.isna()\n",
    "        idx = idx1|idx2\n",
    "        result['n_comment'].loc[idx] = result['n_comment'].loc[idx].apply(lambda x: 0)\n",
    "        result['n_comment'] = result['n_comment'].astype(float)\n",
    "        return result\n",
    "        \n",
    "    \n",
    "    def _str_to_datetype(self,result):\n",
    "        if pd.api.types.is_datetime64_ns_dtype(result['date']):\n",
    "            pass\n",
    "        else:\n",
    "            result['date'] = pd.to_datetime(result['date'])\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def _add_n_hashtag(self,result):\n",
    "        result['n_hashtage'] = 0\n",
    "        idx = result['description'].notnull()\n",
    "        result.loc[idx, 'n_hashtage'] = result.loc[idx, 'description'].apply(lambda x: len(x.split('#'))-1)\n",
    "        return result\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_to_merge(data, numeric, non_numeric):\n",
    "        data = data.reset_index(drop=True)\n",
    "        num_to_add = data.title.shape[0] - data.title.isna().sum()\n",
    "        data = pd.concat((data.loc[0,non_numeric], data[numeric].mean()))\n",
    "        data['video_num'] = num_to_add\n",
    "        return data\n",
    "    def _merge(self, result, non_numeric):\n",
    "        #operate both merge and creating video_num featrue simultaneously.\n",
    "        numeric = [col for col in result.columns.tolist() if col not in non_numeric]\n",
    "        return result.groupby(['channel', 'date']).apply(lambda x: self._get_to_merge(x, numeric, non_numeric)).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_diff(result, feature, new_name):\n",
    "        result = result.reset_index(drop=True)\n",
    "        result[new_name] = (result[feature] - result[feature].shift())\n",
    "        return result\n",
    "    def _add_diff(self, result, feature, new_name):\n",
    "        result = result.groupby('channel').apply(lambda x: self._get_diff(x, feature, new_name)).reset_index(drop=True)\n",
    "        result[new_name] = result[new_name].fillna(0)\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_no_upload_interval(result):\n",
    "        result = result.reset_index(drop=True)\n",
    "        upload_idx = result[result['video_num'] != 0].index.tolist()\n",
    "        temp = [0 for i in range(result.shape[0])]\n",
    "        for i in range(len(upload_idx)):\n",
    "            if i == len(upload_idx)-1:\n",
    "                former = upload_idx[i]\n",
    "                temp[former+1:] = [i+1 for i in range(len(temp[former+1:]))]\n",
    "            else:\n",
    "                former, latter = upload_idx[i], upload_idx[i+1]\n",
    "                temp[former+1:latter] = [i+1 for i in range(len(temp[former+1:latter]))]\n",
    "        result['no_upload_interval'] = temp\n",
    "        return result\n",
    "    def _add_no_upload_interval(self,result):\n",
    "        return result.groupby('channel').apply(lambda x: self._get_no_upload_interval(x)).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    def _remove_nan(self, result):\n",
    "        numeric = [col for col in result.columns.tolist() if col not in self.non_numeric]\n",
    "        result.loc[:, numeric] = result.loc[:,numeric].fillna(0)\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def _one_hot(self, data):\n",
    "        data.loc[:,'genre'] = data.genre.fillna('etc')\n",
    "        genre = data.genre.unique().tolist()\n",
    "        for i, name in enumerate(genre):\n",
    "            data.genre[data.genre==name] = data.genre[data.genre==name].apply(lambda x: i)\n",
    "            \n",
    "        one_hot = pd.get_dummies(data.genre.unique().tolist())\n",
    "        data['one_hot'] = data.genre\n",
    "        for i in range(len(one_hot)):\n",
    "            data.loc[data.genre==i,'one_hot'] = data.loc[data.genre==i, 'genre'].apply(lambda x: one_hot[i].values)\n",
    "    ####################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    #CREATE SEQUENTIAL DATA\n",
    "    ####################################################################\n",
    "    def _extract_at_least_filter(self, result, filter_size):\n",
    "        #fillter_size 이상인 채널 추출하기\n",
    "        alive_idx = result['channel'].value_counts() >= filter_size\n",
    "        alive_array = alive_idx[alive_idx==True].index\n",
    "        return result[result['channel'].isin(alive_array)].reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _to_sequential(result, filter_size, target_size, stride, features, target_features):\n",
    "        result = result.reset_index(drop=True)\n",
    "        idx_list = result.index.tolist()\n",
    "        \n",
    "        train, target = [],[]\n",
    "        for i in range((len(idx_list)-filter_size-target_size)//stride +1):\n",
    "            train_idx = idx_list[i*stride : i*stride + filter_size]\n",
    "            target_idx = idx_list[i*stride + filter_size : i*stride + filter_size + target_size]\n",
    "            train_temp = result.loc[train_idx,:].values.reshape(1,-1)\n",
    "            target_temp = result.loc[target_idx,target_features].values.reshape(1,-1)\n",
    "            \n",
    "            train = train_temp.copy() if i == 0 else np.vstack([train, train_temp])\n",
    "            target = target_temp.copy() if i == 0 else np.vstack([target, target_temp])\n",
    "            \n",
    "        train = pd.DataFrame(train, columns = result.columns.tolist()*filter_size)\n",
    "        target = pd.DataFrame(target, columns = target_features*target_size)\n",
    "        return train[features], target\n",
    "    def _create_sequential_data(self, result, filter_size=7, target_size=1, stride=1, features=None, target_features=None):\n",
    "        #remove channels with few information with respect to filter_size and target_size to extract\n",
    "        result = self._extract_at_least_filter(result, filter_size + target_size)\n",
    "        \n",
    "        #features: features to drop fromf X (features)\n",
    "        #target_features: features to extract from Y (targets)\n",
    "        if features is None:\n",
    "            features = ['date', 'genre','title', 'channel', 'description',\t'sign_in', 'current_cumul_view', 'current_n_video', 'current_cumul_subs']\n",
    "        if target_features is None:\n",
    "            target_features = ['sub_diff']\n",
    "        \n",
    "        #return train, target set wrt groups\n",
    "        result = result.groupby('channel').apply(lambda x: self._to_sequential(x, filter_size, target_size, stride, features, target_features)).reset_index(drop=True)\n",
    "        return self._combine(result)\n",
    "    \n",
    "    \n",
    "    def _combine(self, result):\n",
    "        temp0, temp1 = [], []\n",
    "        for i in range(len(result)):\n",
    "            temp0.append(result[i][0])\n",
    "            temp1.append(result[i][1])\n",
    "        temp0 = pd.concat(temp0)\n",
    "        temp1 = pd.concat(temp1)\n",
    "        return (temp0, temp1)\n",
    "    ####################################################################\n",
    "\n",
    "\n",
    "    \n",
    "    #SCALE\n",
    "    ####################################################################\n",
    "    def scale(self, data, return_original_scale=True):\n",
    "        original_scale = pd.concat((data.max(), data.min()), axis=1).T\n",
    "        original_scale.index=['max', 'min']\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "        data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n",
    "        if return_original_scale:\n",
    "            return data, original_scale\n",
    "        return data\n",
    "    \n",
    "        \n",
    "    def inverse_scale(self, pred, scl):\n",
    "        for idx in range(pred.shape[1]):\n",
    "            pred.iloc[:,idx] = (scl.iloc[0,idx]-scl.iloc[1,idx])*pred.iloc[:,idx]+scl.iloc[1,idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader(Preprocessor):\n",
    "    def __init__(self, path):\n",
    "        self._raw = pd.read_csv(path)\n",
    "        self.data = super()._fit_transform(self._raw.copy())\n",
    "        print('Data Loaded. :P')\n",
    "        \n",
    "        \n",
    "    def get_data(self, filter_size=7, target_size=1, stride=1, features=None, target_features=None,\n",
    "                         channel:list=None, shuffle=False, random_state=None, order=None):\n",
    "        if channel is None:\n",
    "            data_to_extract = self.data\n",
    "        else:\n",
    "            channels = self.list_channel[channel].tolist()\n",
    "            data_to_extract = self.data.set_index('channel').loc[channels].reset_index()\n",
    "            \n",
    "        train, target = self._create_sequential_data(data_to_extract, filter_size, target_size, stride, features, target_features)\n",
    "        if shuffle:\n",
    "            train, target = utils.shuffle(train, target, random_state=random_state)\n",
    "        \n",
    "        if order:\n",
    "            train_col = [col for col in train.columns.unique().tolist()]\n",
    "            target_col = [col for col in target.columns.unique().tolist()]\n",
    "            train, target = train[train_col], target[target_col]\n",
    "            \n",
    "        return train, target\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def list_features(self):\n",
    "        #list the entire features, hence you can choose which features are included in whole set.\n",
    "        return self.data.columns.tolist()\n",
    "    \n",
    "    @property\n",
    "    def list_channel(self):\n",
    "        #list indices of channel.\n",
    "        return pd.Series(self.data.channel.unique().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> load함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(\n",
    "    filter_size: '60, 90, 180', \n",
    "    target_size: '1, 7, 30, 180', \n",
    "    stride: '1, 2, 3',\n",
    "    drop_suffix: '각 변수 끝에 붙은 번호를 제거할지 여부'=True,\n",
    "    path='/home/mskang/CapstoneUOS/notebooks/ModelResearch_iloveslowfood/data_variants'):\n",
    "    \n",
    "    print(f'Setting: filter_size({filter_size})\\ttarget_size({target_size})\\tstride({stride})\\tdrop_suffix({drop_suffix})')\n",
    "    X_name = f'fs({filter_size})_ts({target_size})_st({stride}).csv'\n",
    "    y_name = f'fs({filter_size})_ts({target_size})_st({stride})_label.csv'\n",
    "    \n",
    "    print('Load feature data...', end='\\t')\n",
    "    X = pd.read_csv(os.path.join(path, X_name))\n",
    "    print('loaded!')\n",
    "    print('Load label data...', end='\\t')\n",
    "    y = pd.read_csv(os.path.join(path, y_name))\n",
    "    print('loaded!')\n",
    "    \n",
    "    if drop_suffix:\n",
    "        X.columns = list(map(lambda x: x.split('.')[0], X.columns.tolist()))\n",
    "        y.columns = list(map(lambda x: x.split('.')[0], y.columns.tolist()))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting: filter_size(90)\ttarget_size(90)\tstride(1)\tdrop_suffix(True)\n",
      "Load feature data...\tloaded!\n",
      "Load label data...\tloaded!\n"
     ]
    }
   ],
   "source": [
    "filter_size = 90\n",
    "target_size = 90\n",
    "stride = 1\n",
    "X, y = load(filter_size, target_size, stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Train/Test/VAlid 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "test_size = 0.2\n",
    "\n",
    "X_trn, X_test, y_trn, y_test = train_test_split(\n",
    "    X, y, \n",
    "    shuffle=True, \n",
    "    test_size=test_size, \n",
    "    random_state=random_state\n",
    ") \n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_trn, y_trn, \n",
    "    shuffle=True, \n",
    "    test_size=test_size, \n",
    "    random_state=random_state\n",
    ") \n",
    "\n",
    "### 스케일링 필요 시 다음을 진행(타깃에 대한 스케일링은 진행되지 않음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler()\n",
    "# X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "# X_valid_scaled = pd.DataFrame(scaler.transform(X_valid), columns=X_valid.columns)\n",
    "# X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D TO 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_to_multi(df):\n",
    "    feature_num=len(set(df.columns))\n",
    "    window_num=int(df.shape[1]/feature_num)\n",
    "    sample_num=int(df.shape[0])\n",
    "    temp=np.empty([sample_num,window_num,feature_num])\n",
    "    for i in range(feature_num):\n",
    "        temp[:,:,i]=df.iloc[:,window_num*i:window_num*i+window_num]\n",
    "    \n",
    "    return temp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>duration</th>\n",
       "      <th>duration</th>\n",
       "      <th>duration</th>\n",
       "      <th>duration</th>\n",
       "      <th>duration</th>\n",
       "      <th>duration</th>\n",
       "      <th>duration</th>\n",
       "      <th>duration</th>\n",
       "      <th>duration</th>\n",
       "      <th>...</th>\n",
       "      <th>no_upload_interval</th>\n",
       "      <th>no_upload_interval</th>\n",
       "      <th>no_upload_interval</th>\n",
       "      <th>no_upload_interval</th>\n",
       "      <th>no_upload_interval</th>\n",
       "      <th>no_upload_interval</th>\n",
       "      <th>no_upload_interval</th>\n",
       "      <th>no_upload_interval</th>\n",
       "      <th>no_upload_interval</th>\n",
       "      <th>no_upload_interval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9428</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63582</th>\n",
       "      <td>7.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>60.1800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57155</th>\n",
       "      <td>6.75</td>\n",
       "      <td>4.28</td>\n",
       "      <td>6.723333</td>\n",
       "      <td>6.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.75</td>\n",
       "      <td>6.4875</td>\n",
       "      <td>7.366667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71082</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>86.48</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>40</td>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45695</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2.63</td>\n",
       "      <td>3.050000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.12</td>\n",
       "      <td>3.47</td>\n",
       "      <td>6.4500</td>\n",
       "      <td>5.580000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26516</th>\n",
       "      <td>13.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.73</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.9000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28530</th>\n",
       "      <td>0.00</td>\n",
       "      <td>10.60</td>\n",
       "      <td>10.370000</td>\n",
       "      <td>11.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.17</td>\n",
       "      <td>...</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93036</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93645</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.07</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62145</th>\n",
       "      <td>0.00</td>\n",
       "      <td>6.37</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.38</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65849 rows × 1530 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       duration  duration   duration  duration  duration  duration  duration  \\\n",
       "9428       0.00      0.00   0.000000      0.00      0.00      0.00    0.0000   \n",
       "63582      7.13      0.00   5.750000      0.00      0.00      0.00   60.1800   \n",
       "57155      6.75      4.28   6.723333      6.91      0.00      8.75    6.4875   \n",
       "71082      0.00      0.00   0.000000      0.00      0.00     86.48    0.0000   \n",
       "45695      0.00      2.63   3.050000      0.00      2.12      3.47    6.4500   \n",
       "...         ...       ...        ...       ...       ...       ...       ...   \n",
       "26516     13.83      0.00   0.000000      0.00      7.73      0.00    8.9000   \n",
       "28530      0.00     10.60  10.370000     11.43      0.00      0.00    0.0000   \n",
       "93036      0.00      0.00   0.000000      0.00      0.00      0.00    0.0000   \n",
       "93645      0.00      0.00   0.000000      0.00      0.00     11.07    0.0000   \n",
       "62145      0.00      6.37   0.000000      2.20      0.00     16.38    0.0000   \n",
       "\n",
       "       duration  duration  duration  ...  no_upload_interval  \\\n",
       "9428   0.000000       0.0      0.00  ...                   0   \n",
       "63582  0.000000       0.0      0.00  ...                  30   \n",
       "57155  7.366667       0.0      0.00  ...                   3   \n",
       "71082  0.000000       0.0      0.00  ...                  33   \n",
       "45695  5.580000       0.0      0.00  ...                   3   \n",
       "...         ...       ...       ...  ...                 ...   \n",
       "26516  0.000000       0.0      0.00  ...                   0   \n",
       "28530  0.000000       0.0      9.17  ...                  42   \n",
       "93036  0.000000       0.0      0.00  ...                   0   \n",
       "93645  0.000000       0.0      0.00  ...                   5   \n",
       "62145  0.000000       0.0      0.00  ...                   5   \n",
       "\n",
       "       no_upload_interval  no_upload_interval  no_upload_interval  \\\n",
       "9428                    0                   0                   0   \n",
       "63582                  31                  32                  33   \n",
       "57155                   4                   5                   0   \n",
       "71082                  34                  35                  36   \n",
       "45695                   0                   0                   1   \n",
       "...                   ...                 ...                 ...   \n",
       "26516                   1                   0                   1   \n",
       "28530                   0                   1                   2   \n",
       "93036                   1                   2                   0   \n",
       "93645                   6                   0                   1   \n",
       "62145                   0                   1                   2   \n",
       "\n",
       "       no_upload_interval  no_upload_interval  no_upload_interval  \\\n",
       "9428                    0                   0                   0   \n",
       "63582                  34                  35                  36   \n",
       "57155                   1                   2                   3   \n",
       "71082                  37                  38                  39   \n",
       "45695                   2                   3                   0   \n",
       "...                   ...                 ...                 ...   \n",
       "26516                   0                   1                   0   \n",
       "28530                   3                   4                   5   \n",
       "93036                   1                   2                   3   \n",
       "93645                   2                   3                   4   \n",
       "62145                   3                   4                   5   \n",
       "\n",
       "       no_upload_interval  no_upload_interval  no_upload_interval  \n",
       "9428                    0                   0                   0  \n",
       "63582                  37                  38                  39  \n",
       "57155                   4                   5                   6  \n",
       "71082                  40                  41                  42  \n",
       "45695                   0                   0                   0  \n",
       "...                   ...                 ...                 ...  \n",
       "26516                   1                   0                   1  \n",
       "28530                   6                   0                   1  \n",
       "93036                   4                   0                   1  \n",
       "93645                   5                   6                   0  \n",
       "62145                   0                   1                   2  \n",
       "\n",
       "[65849 rows x 1530 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3d=stack_to_multi(X_train)\n",
    "X_valid_3d=stack_to_multi(X_valid)\n",
    "X_test_3d=stack_to_multi(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65849, 90, 17)\n",
      "(16463, 90, 17)\n",
      "(20579, 90, 17)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_3d.shape)\n",
    "print(X_valid_3d.shape)\n",
    "print(X_test_3d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65849, 90)\n",
      "(16463, 90)\n",
      "(20579, 90)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_valid.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3D 스케일링, 타겟 스케일링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=preprocessing.MinMaxScaler()\n",
    "#scaler_y=preprocessing.MinMaxScaler()\n",
    "def fit_3d(x_train,x_val,x_test):\n",
    "    x_train_sample = x_train.shape[0] #샘플 개수\n",
    "    x_val_sample=x_val.shape[0]\n",
    "    x_test_sample=x_test.shape[0]\n",
    "    \n",
    "    x_timestep = x_train.shape[1] # timestep\n",
    "    x_feature = x_train.shape[2]# feature 차원 \n",
    "    scaler=MinMaxScaler()\n",
    "    for ss in range(x_timestep):\n",
    "        scaler.partial_fit(x_train[:, ss, :]) # 순회피팅\n",
    "\n",
    "    results1,results2,results3=([],[],[])\n",
    "    for ss in range(x_timestep):\n",
    "        results1.append(scaler.transform(x_train[:, ss, :]).reshape(x_train_sample, 1, x_feature))\n",
    "        results2.append(scaler.transform(x_val[:,ss,:]).reshape(x_val_sample,1,x_feature))\n",
    "        results3.append(scaler.transform(x_test[:,ss,:]).reshape(x_test_sample,1,x_feature))\n",
    "    df_train_scaled = np.concatenate(results1, axis=1) #합치기.\n",
    "    df_val_scaled=np.concatenate(results2,axis=1)\n",
    "    df_test_scaled=np.concatenate(results3,axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return df_train_scaled,df_val_scaled,df_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled,X_valid_scaled,X_test_scaled=fit_3d(X_train_3d, X_valid_3d, X_test_3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> target shape 변경 ( 샘플 x 타입스텝 x 예측일수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_subdiff=X_train['sub_diff']\n",
    "X_valid_subdiff=X_valid['sub_diff']\n",
    "X_test_subdiff=X_test['sub_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>...</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "      <th>sub_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9428</th>\n",
       "      <td>4100.0</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>4100.0</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>4100.0</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>4100.0</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>4100.0</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63582</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57155</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71082</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45695</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4401.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26516</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28530</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93036</th>\n",
       "      <td>-10.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93645</th>\n",
       "      <td>200.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62145</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2307.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65849 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sub_diff  sub_diff  sub_diff  sub_diff  sub_diff  sub_diff  sub_diff  \\\n",
       "9428     4100.0    4200.0    4100.0    4200.0    4100.0    4200.0    4100.0   \n",
       "63582       0.0     100.0     100.0     100.0     100.0       0.0       0.0   \n",
       "57155      10.0      10.0      10.0      30.0      15.0      15.0      30.0   \n",
       "71082       5.0      10.0       0.0       0.0      10.0       5.0       5.0   \n",
       "45695       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "26516       0.0       0.0       5.0       5.0       5.0       5.0       0.0   \n",
       "28530       0.0     100.0       0.0       0.0       0.0       0.0       0.0   \n",
       "93036     -10.0     -10.0     -10.0      -8.0      -7.0      -8.0      -7.0   \n",
       "93645     200.0     100.0     100.0       0.0       0.0     100.0     100.0   \n",
       "62145       2.0       2.0       1.0    2307.0      50.0      85.0      85.0   \n",
       "\n",
       "       sub_diff  sub_diff  sub_diff  ...  sub_diff  sub_diff  sub_diff  \\\n",
       "9428     4200.0    4100.0    4200.0  ...   10000.0       0.0       0.0   \n",
       "63582       0.0     100.0       0.0  ...       0.0       0.0       0.0   \n",
       "57155       5.0       5.0      10.0  ...      21.0      20.0      20.0   \n",
       "71082     220.0      50.0      50.0  ...      10.0       0.0       0.0   \n",
       "45695       0.0       0.0       0.0  ...       0.0       0.0       0.0   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "26516       0.0       0.0       0.0  ...       5.0       0.0      10.0   \n",
       "28530       0.0       0.0       0.0  ...       0.0       0.0       0.0   \n",
       "93036      -3.0      -2.0      -3.0  ...      -5.0      -5.0      -5.0   \n",
       "93645     100.0     200.0     100.0  ...       0.0       0.0       0.0   \n",
       "62145      90.0      50.0      60.0  ...      15.0      15.0      30.0   \n",
       "\n",
       "       sub_diff  sub_diff  sub_diff  sub_diff  sub_diff  sub_diff  sub_diff  \n",
       "9428        0.0       0.0   10000.0       0.0   10000.0       0.0       0.0  \n",
       "63582       0.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "57155      20.0      21.0      20.0      20.0      21.0      20.0      20.0  \n",
       "71082     230.0       1.0       1.0       1.0       2.0       1.0       1.0  \n",
       "45695       0.0       0.0    4401.0     800.0       0.0       0.0       0.0  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "26516       6.0       7.0       7.0      10.0      10.0      10.0       6.0  \n",
       "28530       0.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "93036      -3.0      -2.0     -10.0      -5.0      -5.0       0.0       0.0  \n",
       "93645       0.0     500.0     500.0       0.0       0.0       0.0       0.0  \n",
       "62145      70.0      20.0      40.0      15.0      15.0      20.0      50.0  \n",
       "\n",
       "[65849 rows x 90 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_subdiff[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (샘플개수 , (180+30)의 TARGET\n",
    "temp1=pd.concat([X_train_subdiff,y_train],axis=1)\n",
    "temp2=pd.concat([X_valid_subdiff,y_valid],axis=1)\n",
    "temp3=pd.concat([X_test_subdiff,y_test],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (샘플, timestep, targetsize)\n",
    "y_train_3d=np.empty((X_train_subdiff.shape[0],X_train_subdiff.shape[1],90)) # 30 : targetsize\n",
    "y_valid_3d=np.empty((X_valid_subdiff.shape[0],X_valid_subdiff.shape[1],90))\n",
    "y_test_3d=np.empty((X_test_subdiff.shape[0],X_test_subdiff.shape[1],90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(X_train_subdiff.shape[1]): # 180 Timestep\n",
    "    y_train_3d[:,t]=temp1.iloc[:,t:t+90]\n",
    "    y_valid_3d[:,t]=temp2.iloc[:,t:t+90]\n",
    "    y_test_3d[:,t]=temp3.iloc[:,t:t+90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65849, 90, 90)\n",
      "(16463, 90, 90)\n",
      "(20579, 90, 90)\n"
     ]
    }
   ],
   "source": [
    "print(y_train_3d.shape)\n",
    "print(y_valid_3d.shape)\n",
    "print(y_test_3d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> y scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_temp=y_train_3d.reshape(-1,1)\n",
    "y_valid_temp=y_valid_3d.reshape(-1,1)\n",
    "y_test_temp=y_test_3d.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_y=MinMaxScaler()\n",
    "y_train_scaled=scaler_y.fit_transform(y_train_temp)\n",
    "y_val_scaled=scaler_y.transform(y_valid_temp)\n",
    "y_test_scaled=scaler_y.transform(y_test_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_scaled=y_train_scaled.reshape(y_train_3d.shape[0],y_train_3d.shape[1],y_train_3d.shape[2])\n",
    "y_valid_scaled=y_val_scaled.reshape(y_valid_3d.shape[0],y_valid_3d.shape[1],y_valid_3d.shape[2])\n",
    "y_test_scaled=y_test_scaled.reshape(y_test_3d.shape[0],y_test_3d.shape[1],y_test_3d.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65849, 90, 90)\n",
      "(16463, 90, 90)\n",
      "(20579, 90, 90)\n"
     ]
    }
   ],
   "source": [
    "print(y_train_scaled.shape)\n",
    "print(y_valid_scaled.shape)\n",
    "print(y_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 마지막 Timestep의 loss만 중요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_time_step_mse(Y_true, Y_pred):\n",
    "    return keras.metrics.mean_absolute_error(Y_true[:, -1], Y_pred[:, -1]) #### <<- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvGRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='Wavenet(90_90_1)(filter:180).h5'\n",
    "callback_list1 = [tf.keras.callbacks.ModelCheckpoint(filepath='Checkpoint/{}'.format(filename),\n",
    "                                                    monitor='val_last_time_step_mse',\n",
    "                                                    verbose=1,\n",
    "                                                    save_best_only=True,\n",
    "                                                    mode='min'),\n",
    "                 tf.keras.callbacks.EarlyStopping(monitor='val_last_time_step_mse',\n",
    "                                                  patience=8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Full=keras.models.Sequential()\n",
    "model_Full.add(keras.layers.InputLayer(input_shape=[None,17]))\n",
    "for rate in (1,2,4,8)*2:\n",
    "    model_Full.add(keras.layers.Conv1D(filters=180,kernel_size=2,padding=\"causal\",\n",
    "                                 activation='relu',dilation_rate=rate))\n",
    "model_Full.add(keras.layers.Conv1D(filters=90,kernel_size=1))\n",
    "model_Full.compile(loss=\"mae\", optimizer='adam', metrics=[last_time_step_mse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Full.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_Full= model_Full.fit(X_train_scaled, y_train_scaled, epochs=20,\n",
    "                    validation_data=(X_valid_scaled,y_valid_scaled),callbacks=callback_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='ConvGRU(90_90_1).h5'\n",
    "callback_list1 = [tf.keras.callbacks.ModelCheckpoint(filepath='Checkpoint/{}'.format(filename),\n",
    "                                                    monitor='val_last_time_step_mse',\n",
    "                                                    verbose=1,\n",
    "                                                    save_best_only=True,\n",
    "                                                    mode='min'),\n",
    "                 tf.keras.callbacks.EarlyStopping(monitor='val_last_time_step_mse',\n",
    "                                                  patience=15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, None, 180)         12420     \n",
      "_________________________________________________________________\n",
      "gru_12 (GRU)                 (None, None, 180)         195480    \n",
      "_________________________________________________________________\n",
      "gru_13 (GRU)                 (None, None, 180)         195480    \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, None, 90)          16290     \n",
      "=================================================================\n",
      "Total params: 419,670\n",
      "Trainable params: 419,670\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model= keras.models.Sequential([\n",
    "    keras.layers.Conv1D(filters=180, kernel_size=4, strides=2, padding=\"valid\",\n",
    "                        input_shape=[None, 17]),\n",
    "    #-------뭐라도 해보기 ----- OK, !! \n",
    "    #keras.layers.MaxPooling1D(pool_size=1, strides=None),\n",
    "    keras.layers.GRU(180, return_sequences=True),\n",
    "    keras.layers.GRU(180, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(90))\n",
    "    ,\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mae\", optimizer=\"adam\", metrics=[last_time_step_mse])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 0.0017 - last_time_step_mse: 0.0017\n",
      "Epoch 00001: val_last_time_step_mse improved from inf to 0.00112, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 188s 92ms/step - loss: 0.0017 - last_time_step_mse: 0.0017 - val_loss: 0.0011 - val_last_time_step_mse: 0.0011\n",
      "Epoch 2/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 0.0010 - last_time_step_mse: 0.0011\n",
      "Epoch 00002: val_last_time_step_mse improved from 0.00112 to 0.00106, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 222s 108ms/step - loss: 0.0010 - last_time_step_mse: 0.0011 - val_loss: 0.0010 - val_last_time_step_mse: 0.0011\n",
      "Epoch 3/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 9.9487e-04 - last_time_step_mse: 0.0010\n",
      "Epoch 00003: val_last_time_step_mse improved from 0.00106 to 0.00099, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 241s 117ms/step - loss: 9.9487e-04 - last_time_step_mse: 0.0010 - val_loss: 9.3687e-04 - val_last_time_step_mse: 9.8693e-04\n",
      "Epoch 4/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 9.4751e-04 - last_time_step_mse: 9.8575e-04\n",
      "Epoch 00004: val_last_time_step_mse improved from 0.00099 to 0.00096, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 238s 116ms/step - loss: 9.4751e-04 - last_time_step_mse: 9.8575e-04 - val_loss: 9.0076e-04 - val_last_time_step_mse: 9.5881e-04\n",
      "Epoch 5/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 9.2413e-04 - last_time_step_mse: 9.5986e-04\n",
      "Epoch 00005: val_last_time_step_mse improved from 0.00096 to 0.00092, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 241s 117ms/step - loss: 9.2413e-04 - last_time_step_mse: 9.5986e-04 - val_loss: 8.8575e-04 - val_last_time_step_mse: 9.2275e-04\n",
      "Epoch 6/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 9.0974e-04 - last_time_step_mse: 9.4615e-04\n",
      "Epoch 00006: val_last_time_step_mse did not improve from 0.00092\n",
      "2058/2058 [==============================] - 241s 117ms/step - loss: 9.0974e-04 - last_time_step_mse: 9.4615e-04 - val_loss: 9.0818e-04 - val_last_time_step_mse: 9.5568e-04\n",
      "Epoch 7/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 8.9517e-04 - last_time_step_mse: 9.3095e-04 ETA: 0s - loss: 8.9543e-04 - last_time_step_mse: 9.3128\n",
      "Epoch 00007: val_last_time_step_mse did not improve from 0.00092\n",
      "2058/2058 [==============================] - 238s 115ms/step - loss: 8.9517e-04 - last_time_step_mse: 9.3095e-04 - val_loss: 9.0050e-04 - val_last_time_step_mse: 9.4453e-04\n",
      "Epoch 8/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 8.8455e-04 - last_time_step_mse: 9.1878e-04\n",
      "Epoch 00008: val_last_time_step_mse improved from 0.00092 to 0.00091, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 235s 114ms/step - loss: 8.8455e-04 - last_time_step_mse: 9.1878e-04 - val_loss: 8.6556e-04 - val_last_time_step_mse: 9.0943e-04\n",
      "Epoch 9/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 8.8695e-04 - last_time_step_mse: 9.2281e-04\n",
      "Epoch 00009: val_last_time_step_mse improved from 0.00091 to 0.00091, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 246s 120ms/step - loss: 8.8695e-04 - last_time_step_mse: 9.2281e-04 - val_loss: 8.6447e-04 - val_last_time_step_mse: 9.0732e-04\n",
      "Epoch 10/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 8.7349e-04 - last_time_step_mse: 9.0327e-04 ETA: 2s - loss: 8.603\n",
      "Epoch 00010: val_last_time_step_mse did not improve from 0.00091\n",
      "2058/2058 [==============================] - 244s 119ms/step - loss: 8.7349e-04 - last_time_step_mse: 9.0327e-04 - val_loss: 0.0059 - val_last_time_step_mse: 0.0063\n",
      "Epoch 11/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 9.4442e-04 - last_time_step_mse: 9.8120e-04\n",
      "Epoch 00011: val_last_time_step_mse did not improve from 0.00091\n",
      "2058/2058 [==============================] - 248s 120ms/step - loss: 9.4442e-04 - last_time_step_mse: 9.8120e-04 - val_loss: 8.9651e-04 - val_last_time_step_mse: 9.3567e-04\n",
      "Epoch 12/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 8.7324e-04 - last_time_step_mse: 9.0420e-04\n",
      "Epoch 00012: val_last_time_step_mse did not improve from 0.00091\n",
      "2058/2058 [==============================] - 246s 120ms/step - loss: 8.7324e-04 - last_time_step_mse: 9.0420e-04 - val_loss: 8.6663e-04 - val_last_time_step_mse: 9.1290e-04\n",
      "Epoch 13/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 8.6117e-04 - last_time_step_mse: 8.8956e-04\n",
      "Epoch 00013: val_last_time_step_mse improved from 0.00091 to 0.00086, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 252s 122ms/step - loss: 8.6117e-04 - last_time_step_mse: 8.8956e-04 - val_loss: 8.2317e-04 - val_last_time_step_mse: 8.5500e-04\n",
      "Epoch 14/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 8.4643e-04 - last_time_step_mse: 8.7203e-04 ETA: 1s - loss: 8.4701e-04 - last\n",
      "Epoch 00014: val_last_time_step_mse did not improve from 0.00086\n",
      "2058/2058 [==============================] - 237s 115ms/step - loss: 8.4643e-04 - last_time_step_mse: 8.7203e-04 - val_loss: 8.2535e-04 - val_last_time_step_mse: 8.6009e-04\n",
      "Epoch 15/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 8.5481e-04 - last_time_step_mse: 8.8154e-04\n",
      "Epoch 00015: val_last_time_step_mse did not improve from 0.00086\n",
      "2058/2058 [==============================] - 237s 115ms/step - loss: 8.5481e-04 - last_time_step_mse: 8.8154e-04 - val_loss: 8.2666e-04 - val_last_time_step_mse: 8.5715e-04\n",
      "Epoch 16/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 8.3485e-04 - last_time_step_mse: 8.5781e-04\n",
      "Epoch 00016: val_last_time_step_mse improved from 0.00086 to 0.00085, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 242s 117ms/step - loss: 8.3485e-04 - last_time_step_mse: 8.5781e-04 - val_loss: 8.1244e-04 - val_last_time_step_mse: 8.4823e-04\n",
      "Epoch 17/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 8.2850e-04 - last_time_step_mse: 8.5001e-04\n",
      "Epoch 00017: val_last_time_step_mse improved from 0.00085 to 0.00082, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 235s 114ms/step - loss: 8.2850e-04 - last_time_step_mse: 8.5001e-04 - val_loss: 7.9245e-04 - val_last_time_step_mse: 8.1971e-04\n",
      "Epoch 18/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 8.2204e-04 - last_time_step_mse: 8.4277e-04\n",
      "Epoch 00018: val_last_time_step_mse did not improve from 0.00082\n",
      "2058/2058 [==============================] - 234s 114ms/step - loss: 8.2204e-04 - last_time_step_mse: 8.4277e-04 - val_loss: 7.9965e-04 - val_last_time_step_mse: 8.2519e-04\n",
      "Epoch 19/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 8.1702e-04 - last_time_step_mse: 8.3476e-04\n",
      "Epoch 00019: val_last_time_step_mse did not improve from 0.00082\n",
      "2058/2058 [==============================] - 239s 116ms/step - loss: 8.1702e-04 - last_time_step_mse: 8.3476e-04 - val_loss: 8.1560e-04 - val_last_time_step_mse: 8.4094e-04\n",
      "Epoch 20/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 8.0707e-04 - last_time_step_mse: 8.2300e-04\n",
      "Epoch 00020: val_last_time_step_mse did not improve from 0.00082\n",
      "2058/2058 [==============================] - 238s 116ms/step - loss: 8.0707e-04 - last_time_step_mse: 8.2300e-04 - val_loss: 8.0409e-04 - val_last_time_step_mse: 8.2197e-04\n",
      "Epoch 21/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 7.9905e-04 - last_time_step_mse: 8.1239e-04 ETA: 1s - loss: 7.9923e-04 - last_time_step_mse: 8.1245 - ETA: 0s - loss: 7.9923e-04 - last_time_st\n",
      "Epoch 00021: val_last_time_step_mse did not improve from 0.00082\n",
      "2058/2058 [==============================] - 239s 116ms/step - loss: 7.9905e-04 - last_time_step_mse: 8.1239e-04 - val_loss: 8.1047e-04 - val_last_time_step_mse: 8.2545e-04\n",
      "Epoch 22/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2058/2058 [==============================] - ETA: 0s - loss: 7.9098e-04 - last_time_step_mse: 8.0314e-04\n",
      "Epoch 00022: val_last_time_step_mse improved from 0.00082 to 0.00079, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 232s 113ms/step - loss: 7.9098e-04 - last_time_step_mse: 8.0314e-04 - val_loss: 7.8054e-04 - val_last_time_step_mse: 7.9322e-04\n",
      "Epoch 23/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 7.8646e-04 - last_time_step_mse: 7.9548e-04 - ETA: 23s - loss: 7.8713e-04 - ETA: 20s - loss: 7.8803e-04 - last_time_step_mse: 7.94 - ETA: 19s - loss: 7.8794e-04 - last_time_step_mse: - ETA: 18s - loss: 7.8785e-04 -  - ETA:\n",
      "Epoch 00023: val_last_time_step_mse improved from 0.00079 to 0.00078, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 231s 112ms/step - loss: 7.8646e-04 - last_time_step_mse: 7.9548e-04 - val_loss: 7.6230e-04 - val_last_time_step_mse: 7.7659e-04\n",
      "Epoch 24/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 7.7801e-04 - last_time_step_mse: 7.8468e-04 ETA: 5s - loss: 7.7\n",
      "Epoch 00024: val_last_time_step_mse improved from 0.00078 to 0.00077, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 236s 115ms/step - loss: 7.7801e-04 - last_time_step_mse: 7.8468e-04 - val_loss: 7.6092e-04 - val_last_time_step_mse: 7.6842e-04\n",
      "Epoch 25/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 7.6978e-04 - last_time_step_mse: 7.7419e-04\n",
      "Epoch 00025: val_last_time_step_mse improved from 0.00077 to 0.00075, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 240s 116ms/step - loss: 7.6978e-04 - last_time_step_mse: 7.7419e-04 - val_loss: 7.4362e-04 - val_last_time_step_mse: 7.5108e-04\n",
      "Epoch 26/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 7.6390e-04 - last_time_step_mse: 7.6600e-04\n",
      "Epoch 00026: val_last_time_step_mse improved from 0.00075 to 0.00074, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 230s 112ms/step - loss: 7.6390e-04 - last_time_step_mse: 7.6600e-04 - val_loss: 7.3794e-04 - val_last_time_step_mse: 7.4087e-04\n",
      "Epoch 27/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 7.5653e-04 - last_time_step_mse: 7.5713e-04\n",
      "Epoch 00027: val_last_time_step_mse did not improve from 0.00074\n",
      "2058/2058 [==============================] - 234s 114ms/step - loss: 7.5653e-04 - last_time_step_mse: 7.5713e-04 - val_loss: 7.4893e-04 - val_last_time_step_mse: 7.5287e-04\n",
      "Epoch 28/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 7.4973e-04 - last_time_step_mse: 7.4847e-04\n",
      "Epoch 00028: val_last_time_step_mse did not improve from 0.00074\n",
      "2058/2058 [==============================] - 234s 114ms/step - loss: 7.4973e-04 - last_time_step_mse: 7.4847e-04 - val_loss: 7.5415e-04 - val_last_time_step_mse: 7.5866e-04\n",
      "Epoch 29/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 7.4073e-04 - last_time_step_mse: 7.3764e-04\n",
      "Epoch 00029: val_last_time_step_mse improved from 0.00074 to 0.00073, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 236s 115ms/step - loss: 7.4073e-04 - last_time_step_mse: 7.3764e-04 - val_loss: 7.4337e-04 - val_last_time_step_mse: 7.3202e-04\n",
      "Epoch 30/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 7.3586e-04 - last_time_step_mse: 7.3118e-04\n",
      "Epoch 00030: val_last_time_step_mse improved from 0.00073 to 0.00071, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 236s 115ms/step - loss: 7.3586e-04 - last_time_step_mse: 7.3118e-04 - val_loss: 7.1836e-04 - val_last_time_step_mse: 7.1473e-04\n",
      "Epoch 31/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 7.3066e-04 - last_time_step_mse: 7.2512e-04 ETA: 2s - ETA: 0s - loss: 7.3068e-04 - last_time_step_mse: 7.2509e-04\n",
      "Epoch 00031: val_last_time_step_mse improved from 0.00071 to 0.00071, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 232s 113ms/step - loss: 7.3068e-04 - last_time_step_mse: 7.2509e-04 - val_loss: 7.1715e-04 - val_last_time_step_mse: 7.1092e-04\n",
      "Epoch 32/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 7.2524e-04 - last_time_step_mse: 7.1641e-04\n",
      "Epoch 00032: val_last_time_step_mse improved from 0.00071 to 0.00070, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 237s 115ms/step - loss: 7.2524e-04 - last_time_step_mse: 7.1641e-04 - val_loss: 7.0859e-04 - val_last_time_step_mse: 7.0492e-04\n",
      "Epoch 33/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 7.2161e-04 - last_time_step_mse: 7.1185e-04\n",
      "Epoch 00033: val_last_time_step_mse did not improve from 0.00070\n",
      "2058/2058 [==============================] - 231s 112ms/step - loss: 7.2161e-04 - last_time_step_mse: 7.1185e-04 - val_loss: 7.3917e-04 - val_last_time_step_mse: 7.2776e-04\n",
      "Epoch 34/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 7.1437e-04 - last_time_step_mse: 7.0315e-04\n",
      "Epoch 00034: val_last_time_step_mse improved from 0.00070 to 0.00070, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 236s 115ms/step - loss: 7.1437e-04 - last_time_step_mse: 7.0315e-04 - val_loss: 7.0493e-04 - val_last_time_step_mse: 7.0255e-04\n",
      "Epoch 35/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 7.0759e-04 - last_time_step_mse: 6.9490e-04\n",
      "Epoch 00035: val_last_time_step_mse improved from 0.00070 to 0.00068, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 236s 115ms/step - loss: 7.0759e-04 - last_time_step_mse: 6.9490e-04 - val_loss: 6.8662e-04 - val_last_time_step_mse: 6.7507e-04\n",
      "Epoch 36/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 7.0757e-04 - last_time_step_mse: 6.9483e-04\n",
      "Epoch 00036: val_last_time_step_mse did not improve from 0.00068\n",
      "2058/2058 [==============================] - 227s 110ms/step - loss: 7.0757e-04 - last_time_step_mse: 6.9483e-04 - val_loss: 7.3474e-04 - val_last_time_step_mse: 7.1583e-04\n",
      "Epoch 37/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 7.0226e-04 - last_time_step_mse: 6.8823e-04 ETA: 0s - loss: 7.0166e-04 - last_time_st\n",
      "Epoch 00037: val_last_time_step_mse did not improve from 0.00068\n",
      "2058/2058 [==============================] - 233s 113ms/step - loss: 7.0226e-04 - last_time_step_mse: 6.8823e-04 - val_loss: 6.9097e-04 - val_last_time_step_mse: 6.7890e-04\n",
      "Epoch 38/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.9652e-04 - last_time_step_mse: 6.8116e-04\n",
      "Epoch 00038: val_last_time_step_mse did not improve from 0.00068\n",
      "2058/2058 [==============================] - 238s 116ms/step - loss: 6.9652e-04 - last_time_step_mse: 6.8116e-04 - val_loss: 7.0818e-04 - val_last_time_step_mse: 6.9616e-04\n",
      "Epoch 39/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.9367e-04 - last_time_step_mse: 6.7815e-04\n",
      "Epoch 00039: val_last_time_step_mse did not improve from 0.00068\n",
      "2058/2058 [==============================] - 233s 113ms/step - loss: 6.9367e-04 - last_time_step_mse: 6.7815e-04 - val_loss: 6.9888e-04 - val_last_time_step_mse: 6.8648e-04\n",
      "Epoch 40/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.8936e-04 - last_time_step_mse: 6.7312e-04\n",
      "Epoch 00040: val_last_time_step_mse did not improve from 0.00068\n",
      "2058/2058 [==============================] - 230s 112ms/step - loss: 6.8936e-04 - last_time_step_mse: 6.7312e-04 - val_loss: 6.9219e-04 - val_last_time_step_mse: 6.8695e-04\n",
      "Epoch 41/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.8544e-04 - last_time_step_mse: 6.6767e-04 - ETA: 16s - lo - ETA: 7s - loss: 6.8500e-04 - last_time_step_mse: 6.6522e- - ETA: 7s - loss: 6.8492e-04 - last_time_step_mse\n",
      "Epoch 00041: val_last_time_step_mse improved from 0.00068 to 0.00067, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 236s 115ms/step - loss: 6.8544e-04 - last_time_step_mse: 6.6767e-04 - val_loss: 6.8086e-04 - val_last_time_step_mse: 6.6716e-04\n",
      "Epoch 42/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2058/2058 [==============================] - ETA: 0s - loss: 6.8350e-04 - last_time_step_mse: 6.6532e-04\n",
      "Epoch 00042: val_last_time_step_mse did not improve from 0.00067\n",
      "2058/2058 [==============================] - 230s 112ms/step - loss: 6.8350e-04 - last_time_step_mse: 6.6532e-04 - val_loss: 6.9009e-04 - val_last_time_step_mse: 6.7463e-04\n",
      "Epoch 43/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.8164e-04 - last_time_step_mse: 6.6349e-04 ETA: 2s -\n",
      "Epoch 00043: val_last_time_step_mse did not improve from 0.00067\n",
      "2058/2058 [==============================] - 230s 112ms/step - loss: 6.8164e-04 - last_time_step_mse: 6.6349e-04 - val_loss: 6.9633e-04 - val_last_time_step_mse: 6.7885e-04\n",
      "Epoch 44/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.7647e-04 - last_time_step_mse: 6.5639e-04\n",
      "Epoch 00044: val_last_time_step_mse improved from 0.00067 to 0.00066, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 210s 102ms/step - loss: 6.7647e-04 - last_time_step_mse: 6.5639e-04 - val_loss: 6.7864e-04 - val_last_time_step_mse: 6.6015e-04\n",
      "Epoch 45/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.7597e-04 - last_time_step_mse: 6.5579e-04\n",
      "Epoch 00045: val_last_time_step_mse improved from 0.00066 to 0.00065, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 201s 98ms/step - loss: 6.7597e-04 - last_time_step_mse: 6.5579e-04 - val_loss: 6.6693e-04 - val_last_time_step_mse: 6.5213e-04\n",
      "Epoch 46/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.7192e-04 - last_time_step_mse: 6.5155e-04\n",
      "Epoch 00046: val_last_time_step_mse did not improve from 0.00065\n",
      "2058/2058 [==============================] - 204s 99ms/step - loss: 6.7192e-04 - last_time_step_mse: 6.5155e-04 - val_loss: 6.8733e-04 - val_last_time_step_mse: 6.7494e-04\n",
      "Epoch 47/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.6828e-04 - last_time_step_mse: 6.4728e-04\n",
      "Epoch 00047: val_last_time_step_mse did not improve from 0.00065\n",
      "2058/2058 [==============================] - 198s 96ms/step - loss: 6.6828e-04 - last_time_step_mse: 6.4728e-04 - val_loss: 6.7195e-04 - val_last_time_step_mse: 6.5599e-04\n",
      "Epoch 48/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.6779e-04 - last_time_step_mse: 6.4717e-04\n",
      "Epoch 00048: val_last_time_step_mse did not improve from 0.00065\n",
      "2058/2058 [==============================] - 200s 97ms/step - loss: 6.6779e-04 - last_time_step_mse: 6.4717e-04 - val_loss: 6.6827e-04 - val_last_time_step_mse: 6.5505e-04\n",
      "Epoch 49/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.6439e-04 - last_time_step_mse: 6.4292e-04\n",
      "Epoch 00049: val_last_time_step_mse did not improve from 0.00065\n",
      "2058/2058 [==============================] - 199s 97ms/step - loss: 6.6439e-04 - last_time_step_mse: 6.4292e-04 - val_loss: 6.7558e-04 - val_last_time_step_mse: 6.5543e-04\n",
      "Epoch 50/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.6053e-04 - last_time_step_mse: 6.3867e-04\n",
      "Epoch 00050: val_last_time_step_mse improved from 0.00065 to 0.00065, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 198s 96ms/step - loss: 6.6053e-04 - last_time_step_mse: 6.3867e-04 - val_loss: 6.6390e-04 - val_last_time_step_mse: 6.4643e-04\n",
      "Epoch 51/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.6109e-04 - last_time_step_mse: 6.3971e-04\n",
      "Epoch 00051: val_last_time_step_mse did not improve from 0.00065\n",
      "2058/2058 [==============================] - 198s 96ms/step - loss: 6.6109e-04 - last_time_step_mse: 6.3971e-04 - val_loss: 6.6953e-04 - val_last_time_step_mse: 6.5578e-04\n",
      "Epoch 52/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.5752e-04 - last_time_step_mse: 6.3497e-04\n",
      "Epoch 00052: val_last_time_step_mse improved from 0.00065 to 0.00063, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 202s 98ms/step - loss: 6.5752e-04 - last_time_step_mse: 6.3497e-04 - val_loss: 6.4569e-04 - val_last_time_step_mse: 6.2844e-04\n",
      "Epoch 53/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.5721e-04 - last_time_step_mse: 6.3503e-04\n",
      "Epoch 00053: val_last_time_step_mse did not improve from 0.00063\n",
      "2058/2058 [==============================] - 201s 98ms/step - loss: 6.5721e-04 - last_time_step_mse: 6.3503e-04 - val_loss: 6.6221e-04 - val_last_time_step_mse: 6.4428e-04\n",
      "Epoch 54/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.5238e-04 - last_time_step_mse: 6.2888e-04\n",
      "Epoch 00054: val_last_time_step_mse did not improve from 0.00063\n",
      "2058/2058 [==============================] - 199s 97ms/step - loss: 6.5238e-04 - last_time_step_mse: 6.2888e-04 - val_loss: 6.6046e-04 - val_last_time_step_mse: 6.3648e-04\n",
      "Epoch 55/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.5249e-04 - last_time_step_mse: 6.2939e-04\n",
      "Epoch 00055: val_last_time_step_mse improved from 0.00063 to 0.00062, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 201s 98ms/step - loss: 6.5249e-04 - last_time_step_mse: 6.2939e-04 - val_loss: 6.4015e-04 - val_last_time_step_mse: 6.2000e-04\n",
      "Epoch 56/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.5015e-04 - last_time_step_mse: 6.2667e-04\n",
      "Epoch 00056: val_last_time_step_mse did not improve from 0.00062\n",
      "2058/2058 [==============================] - 200s 97ms/step - loss: 6.5015e-04 - last_time_step_mse: 6.2667e-04 - val_loss: 6.5042e-04 - val_last_time_step_mse: 6.2872e-04\n",
      "Epoch 57/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.4801e-04 - last_time_step_mse: 6.2411e-04\n",
      "Epoch 00057: val_last_time_step_mse did not improve from 0.00062\n",
      "2058/2058 [==============================] - 200s 97ms/step - loss: 6.4801e-04 - last_time_step_mse: 6.2411e-04 - val_loss: 6.4652e-04 - val_last_time_step_mse: 6.2661e-04\n",
      "Epoch 58/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.4663e-04 - last_time_step_mse: 6.2205e-04\n",
      "Epoch 00058: val_last_time_step_mse improved from 0.00062 to 0.00061, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 201s 97ms/step - loss: 6.4663e-04 - last_time_step_mse: 6.2205e-04 - val_loss: 6.3309e-04 - val_last_time_step_mse: 6.1132e-04\n",
      "Epoch 59/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.6283e-04 - last_time_step_mse: 6.4323e-04 ETA: 3s - loss: 6.6358e-04 - last_tim\n",
      "Epoch 00059: val_last_time_step_mse did not improve from 0.00061\n",
      "2058/2058 [==============================] - 200s 97ms/step - loss: 6.6283e-04 - last_time_step_mse: 6.4323e-04 - val_loss: 6.4076e-04 - val_last_time_step_mse: 6.1465e-04\n",
      "Epoch 60/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.4020e-04 - last_time_step_mse: 6.1411e-04\n",
      "Epoch 00060: val_last_time_step_mse did not improve from 0.00061\n",
      "2058/2058 [==============================] - 201s 98ms/step - loss: 6.4020e-04 - last_time_step_mse: 6.1411e-04 - val_loss: 6.4303e-04 - val_last_time_step_mse: 6.2187e-04\n",
      "Epoch 61/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.4252e-04 - last_time_step_mse: 6.1696e-04\n",
      "Epoch 00061: val_last_time_step_mse improved from 0.00061 to 0.00061, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 207s 100ms/step - loss: 6.4252e-04 - last_time_step_mse: 6.1696e-04 - val_loss: 6.3196e-04 - val_last_time_step_mse: 6.0956e-04\n",
      "Epoch 62/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.3938e-04 - last_time_step_mse: 6.1370e-04\n",
      "Epoch 00062: val_last_time_step_mse improved from 0.00061 to 0.00061, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 206s 100ms/step - loss: 6.3938e-04 - last_time_step_mse: 6.1370e-04 - val_loss: 6.3516e-04 - val_last_time_step_mse: 6.0849e-04\n",
      "Epoch 63/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.3958e-04 - last_time_step_mse: 6.1397e-04\n",
      "Epoch 00063: val_last_time_step_mse did not improve from 0.00061\n",
      "2058/2058 [==============================] - 200s 97ms/step - loss: 6.3958e-04 - last_time_step_mse: 6.1397e-04 - val_loss: 6.5484e-04 - val_last_time_step_mse: 6.2375e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.4073e-04 - last_time_step_mse: 6.1549e-04\n",
      "Epoch 00064: val_last_time_step_mse did not improve from 0.00061\n",
      "2058/2058 [==============================] - 203s 99ms/step - loss: 6.4073e-04 - last_time_step_mse: 6.1549e-04 - val_loss: 6.4261e-04 - val_last_time_step_mse: 6.2160e-04\n",
      "Epoch 65/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.3630e-04 - last_time_step_mse: 6.1010e-04\n",
      "Epoch 00065: val_last_time_step_mse did not improve from 0.00061\n",
      "2058/2058 [==============================] - 201s 98ms/step - loss: 6.3630e-04 - last_time_step_mse: 6.1010e-04 - val_loss: 6.4255e-04 - val_last_time_step_mse: 6.1664e-04\n",
      "Epoch 66/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.3333e-04 - last_time_step_mse: 6.0707e-04\n",
      "Epoch 00066: val_last_time_step_mse did not improve from 0.00061\n",
      "2058/2058 [==============================] - 199s 97ms/step - loss: 6.3333e-04 - last_time_step_mse: 6.0707e-04 - val_loss: 6.3290e-04 - val_last_time_step_mse: 6.0909e-04\n",
      "Epoch 67/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.3608e-04 - last_time_step_mse: 6.1043e-04\n",
      "Epoch 00067: val_last_time_step_mse improved from 0.00061 to 0.00061, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 201s 97ms/step - loss: 6.3608e-04 - last_time_step_mse: 6.1043e-04 - val_loss: 6.3445e-04 - val_last_time_step_mse: 6.0537e-04\n",
      "Epoch 68/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.3343e-04 - last_time_step_mse: 6.0739e-04\n",
      "Epoch 00068: val_last_time_step_mse did not improve from 0.00061\n",
      "2058/2058 [==============================] - 198s 96ms/step - loss: 6.3343e-04 - last_time_step_mse: 6.0739e-04 - val_loss: 6.5821e-04 - val_last_time_step_mse: 6.3643e-04\n",
      "Epoch 69/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.3704e-04 - last_time_step_mse: 6.1336e-04\n",
      "Epoch 00069: val_last_time_step_mse improved from 0.00061 to 0.00060, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 202s 98ms/step - loss: 6.3704e-04 - last_time_step_mse: 6.1336e-04 - val_loss: 6.2753e-04 - val_last_time_step_mse: 6.0379e-04\n",
      "Epoch 70/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.2975e-04 - last_time_step_mse: 6.0284e-04\n",
      "Epoch 00070: val_last_time_step_mse did not improve from 0.00060\n",
      "2058/2058 [==============================] - 204s 99ms/step - loss: 6.2975e-04 - last_time_step_mse: 6.0284e-04 - val_loss: 6.3039e-04 - val_last_time_step_mse: 6.0504e-04\n",
      "Epoch 71/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.4825e-04 - last_time_step_mse: 6.2910e-04\n",
      "Epoch 00071: val_last_time_step_mse did not improve from 0.00060\n",
      "2058/2058 [==============================] - 202s 98ms/step - loss: 6.4825e-04 - last_time_step_mse: 6.2910e-04 - val_loss: 6.3977e-04 - val_last_time_step_mse: 6.2307e-04\n",
      "Epoch 72/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.3353e-04 - last_time_step_mse: 6.0897e-04\n",
      "Epoch 00072: val_last_time_step_mse did not improve from 0.00060\n",
      "2058/2058 [==============================] - 198s 96ms/step - loss: 6.3353e-04 - last_time_step_mse: 6.0897e-04 - val_loss: 6.3048e-04 - val_last_time_step_mse: 6.0411e-04\n",
      "Epoch 73/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.2753e-04 - last_time_step_mse: 6.0161e-04\n",
      "Epoch 00073: val_last_time_step_mse did not improve from 0.00060\n",
      "2058/2058 [==============================] - 203s 99ms/step - loss: 6.2753e-04 - last_time_step_mse: 6.0161e-04 - val_loss: 6.3116e-04 - val_last_time_step_mse: 6.0844e-04\n",
      "Epoch 74/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.2398e-04 - last_time_step_mse: 5.9697e-04\n",
      "Epoch 00074: val_last_time_step_mse improved from 0.00060 to 0.00060, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 201s 98ms/step - loss: 6.2398e-04 - last_time_step_mse: 5.9697e-04 - val_loss: 6.2427e-04 - val_last_time_step_mse: 5.9959e-04\n",
      "Epoch 75/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.2742e-04 - last_time_step_mse: 6.0092e-04\n",
      "Epoch 00075: val_last_time_step_mse improved from 0.00060 to 0.00059, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 208s 101ms/step - loss: 6.2742e-04 - last_time_step_mse: 6.0092e-04 - val_loss: 6.1481e-04 - val_last_time_step_mse: 5.8706e-04\n",
      "Epoch 76/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.2346e-04 - last_time_step_mse: 5.9726e-04\n",
      "Epoch 00076: val_last_time_step_mse did not improve from 0.00059\n",
      "2058/2058 [==============================] - 202s 98ms/step - loss: 6.2346e-04 - last_time_step_mse: 5.9726e-04 - val_loss: 6.1933e-04 - val_last_time_step_mse: 5.9284e-04\n",
      "Epoch 77/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.2190e-04 - last_time_step_mse: 5.9462e-04\n",
      "Epoch 00077: val_last_time_step_mse improved from 0.00059 to 0.00059, saving model to Checkpoint/ConvGRU(90_90_1).h5\n",
      "2058/2058 [==============================] - 201s 98ms/step - loss: 6.2190e-04 - last_time_step_mse: 5.9462e-04 - val_loss: 6.1305e-04 - val_last_time_step_mse: 5.8661e-04\n",
      "Epoch 78/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.2057e-04 - last_time_step_mse: 5.9342e-04\n",
      "Epoch 00078: val_last_time_step_mse did not improve from 0.00059\n",
      "2058/2058 [==============================] - 201s 98ms/step - loss: 6.2057e-04 - last_time_step_mse: 5.9342e-04 - val_loss: 6.1673e-04 - val_last_time_step_mse: 5.9056e-04\n",
      "Epoch 79/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.1919e-04 - last_time_step_mse: 5.9173e-04\n",
      "Epoch 00079: val_last_time_step_mse did not improve from 0.00059\n",
      "2058/2058 [==============================] - 186s 90ms/step - loss: 6.1919e-04 - last_time_step_mse: 5.9173e-04 - val_loss: 6.4449e-04 - val_last_time_step_mse: 6.0788e-04\n",
      "Epoch 80/80\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.1928e-04 - last_time_step_mse: 5.9224e-04\n",
      "Epoch 00080: val_last_time_step_mse did not improve from 0.00059\n",
      "2058/2058 [==============================] - 177s 86ms/step - loss: 6.1928e-04 - last_time_step_mse: 5.9224e-04 - val_loss: 6.4538e-04 - val_last_time_step_mse: 6.1774e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train_scaled[:, 3::2], epochs=80,\n",
    "                    validation_data=(X_valid_scaled,y_valid_scaled[:, 3::2]),callbacks=callback_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['last_time_step_mse'], label='train_loss')\n",
    "plt.plot(history.history['val_last_time_step_mse'], label='val_loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.123475382979799"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.mean(keras.metrics.mean_squared_error([[3],[1],[3],[4]], [[1],[5],[10],[10]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 추가로 돌려 볼 가치 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='Wavenet(90_90_1)(filter:180)(plus2).h5'\n",
    "callback_list1 = [tf.keras.callbacks.ModelCheckpoint(filepath='Checkpoint/{}'.format(filename),\n",
    "                                                    monitor='val_last_time_step_mse',\n",
    "                                                    verbose=1,\n",
    "                                                    save_best_only=True,\n",
    "                                                    mode='min'),\n",
    "                 tf.keras.callbacks.EarlyStopping(monitor='val_last_time_step_mse',\n",
    "                                                  patience=15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtersize=90\n",
    "targetsize=90\n",
    "stride=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='Checkpoint'+'/Wavenet({}_{}_{})'.format(filtersize,targetsize,stride)+'(filter:180).h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model(path,custom_objects={'customLoss1':last_time_step_mse},compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "loaded_model.compile(optimizer='adam', loss='mae',metrics=[last_time_step_mse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 7.0500e-04 - last_time_step_mse: 7.2569e-04\n",
      "Epoch 00001: val_last_time_step_mse improved from inf to 0.00075, saving model to Checkpoint/Wavenet(90_90_1)(filter:180)(plus).h5\n",
      "2058/2058 [==============================] - 108s 53ms/step - loss: 7.0490e-04 - last_time_step_mse: 7.2558e-04 - val_loss: 7.2368e-04 - val_last_time_step_mse: 7.5155e-04\n",
      "Epoch 2/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 7.0092e-04 - last_time_step_mse: 7.2176e-04\n",
      "Epoch 00002: val_last_time_step_mse improved from 0.00075 to 0.00071, saving model to Checkpoint/Wavenet(90_90_1)(filter:180)(plus).h5\n",
      "2058/2058 [==============================] - 97s 47ms/step - loss: 7.0093e-04 - last_time_step_mse: 7.2172e-04 - val_loss: 6.8548e-04 - val_last_time_step_mse: 7.0905e-04\n",
      "Epoch 3/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.9963e-04 - last_time_step_mse: 7.1908e-04\n",
      "Epoch 00003: val_last_time_step_mse did not improve from 0.00071\n",
      "2058/2058 [==============================] - 93s 45ms/step - loss: 6.9963e-04 - last_time_step_mse: 7.1908e-04 - val_loss: 7.0077e-04 - val_last_time_step_mse: 7.2984e-04\n",
      "Epoch 4/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.9699e-04 - last_time_step_mse: 7.1699e-04\n",
      "Epoch 00004: val_last_time_step_mse improved from 0.00071 to 0.00070, saving model to Checkpoint/Wavenet(90_90_1)(filter:180)(plus).h5\n",
      "2058/2058 [==============================] - 94s 45ms/step - loss: 6.9699e-04 - last_time_step_mse: 7.1699e-04 - val_loss: 6.7747e-04 - val_last_time_step_mse: 7.0305e-04\n",
      "Epoch 5/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.9552e-04 - last_time_step_mse: 7.1575e-04\n",
      "Epoch 00005: val_last_time_step_mse did not improve from 0.00070\n",
      "2058/2058 [==============================] - 93s 45ms/step - loss: 6.9554e-04 - last_time_step_mse: 7.1578e-04 - val_loss: 6.7591e-04 - val_last_time_step_mse: 7.0422e-04\n",
      "Epoch 6/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.9018e-04 - last_time_step_mse: 7.1020e-04\n",
      "Epoch 00006: val_last_time_step_mse improved from 0.00070 to 0.00069, saving model to Checkpoint/Wavenet(90_90_1)(filter:180)(plus).h5\n",
      "2058/2058 [==============================] - 95s 46ms/step - loss: 6.9016e-04 - last_time_step_mse: 7.1015e-04 - val_loss: 6.6914e-04 - val_last_time_step_mse: 6.9467e-04\n",
      "Epoch 7/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.8714e-04 - last_time_step_mse: 7.0791e-04\n",
      "Epoch 00007: val_last_time_step_mse improved from 0.00069 to 0.00069, saving model to Checkpoint/Wavenet(90_90_1)(filter:180)(plus).h5\n",
      "2058/2058 [==============================] - 92s 45ms/step - loss: 6.8714e-04 - last_time_step_mse: 7.0791e-04 - val_loss: 6.6301e-04 - val_last_time_step_mse: 6.8886e-04\n",
      "Epoch 8/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.8389e-04 - last_time_step_mse: 7.0387e-04\n",
      "Epoch 00008: val_last_time_step_mse improved from 0.00069 to 0.00069, saving model to Checkpoint/Wavenet(90_90_1)(filter:180)(plus).h5\n",
      "2058/2058 [==============================] - 93s 45ms/step - loss: 6.8387e-04 - last_time_step_mse: 7.0380e-04 - val_loss: 6.6165e-04 - val_last_time_step_mse: 6.8611e-04\n",
      "Epoch 9/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.8085e-04 - last_time_step_mse: 7.0050e-04\n",
      "Epoch 00009: val_last_time_step_mse did not improve from 0.00069\n",
      "2058/2058 [==============================] - 92s 45ms/step - loss: 6.8085e-04 - last_time_step_mse: 7.0050e-04 - val_loss: 7.1001e-04 - val_last_time_step_mse: 7.3900e-04\n",
      "Epoch 10/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.7781e-04 - last_time_step_mse: 6.9738e-04\n",
      "Epoch 00010: val_last_time_step_mse improved from 0.00069 to 0.00068, saving model to Checkpoint/Wavenet(90_90_1)(filter:180)(plus).h5\n",
      "2058/2058 [==============================] - 89s 43ms/step - loss: 6.7769e-04 - last_time_step_mse: 6.9725e-04 - val_loss: 6.5571e-04 - val_last_time_step_mse: 6.8220e-04\n",
      "Epoch 11/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.7408e-04 - last_time_step_mse: 6.9378e-04\n",
      "Epoch 00011: val_last_time_step_mse did not improve from 0.00068\n",
      "2058/2058 [==============================] - 93s 45ms/step - loss: 6.7420e-04 - last_time_step_mse: 6.9399e-04 - val_loss: 6.6146e-04 - val_last_time_step_mse: 6.8730e-04\n",
      "Epoch 12/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.7271e-04 - last_time_step_mse: 6.9231e-04\n",
      "Epoch 00012: val_last_time_step_mse did not improve from 0.00068\n",
      "2058/2058 [==============================] - 94s 46ms/step - loss: 6.7271e-04 - last_time_step_mse: 6.9231e-04 - val_loss: 6.6569e-04 - val_last_time_step_mse: 6.9264e-04\n",
      "Epoch 13/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.7385e-04 - last_time_step_mse: 6.9427e-04\n",
      "Epoch 00013: val_last_time_step_mse did not improve from 0.00068\n",
      "2058/2058 [==============================] - 91s 44ms/step - loss: 6.7385e-04 - last_time_step_mse: 6.9427e-04 - val_loss: 6.6399e-04 - val_last_time_step_mse: 6.9234e-04\n",
      "Epoch 14/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.6913e-04 - last_time_step_mse: 6.8966e-04\n",
      "Epoch 00014: val_last_time_step_mse did not improve from 0.00068\n",
      "2058/2058 [==============================] - 91s 44ms/step - loss: 6.6909e-04 - last_time_step_mse: 6.8959e-04 - val_loss: 6.8571e-04 - val_last_time_step_mse: 7.1116e-04\n",
      "Epoch 15/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.6603e-04 - last_time_step_mse: 6.8613e-04\n",
      "Epoch 00015: val_last_time_step_mse improved from 0.00068 to 0.00067, saving model to Checkpoint/Wavenet(90_90_1)(filter:180)(plus).h5\n",
      "2058/2058 [==============================] - 91s 44ms/step - loss: 6.6595e-04 - last_time_step_mse: 6.8620e-04 - val_loss: 6.5111e-04 - val_last_time_step_mse: 6.7460e-04\n",
      "Epoch 16/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.6423e-04 - last_time_step_mse: 6.8407e-04\n",
      "Epoch 00016: val_last_time_step_mse did not improve from 0.00067\n",
      "2058/2058 [==============================] - 91s 44ms/step - loss: 6.6411e-04 - last_time_step_mse: 6.8393e-04 - val_loss: 6.5713e-04 - val_last_time_step_mse: 6.8014e-04\n",
      "Epoch 17/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.6094e-04 - last_time_step_mse: 6.8031e-04\n",
      "Epoch 00017: val_last_time_step_mse did not improve from 0.00067\n",
      "2058/2058 [==============================] - 91s 44ms/step - loss: 6.6094e-04 - last_time_step_mse: 6.8031e-04 - val_loss: 6.6802e-04 - val_last_time_step_mse: 6.9708e-04\n",
      "Epoch 18/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.6198e-04 - last_time_step_mse: 6.8199e-04\n",
      "Epoch 00018: val_last_time_step_mse improved from 0.00067 to 0.00067, saving model to Checkpoint/Wavenet(90_90_1)(filter:180)(plus).h5\n",
      "2058/2058 [==============================] - 90s 44ms/step - loss: 6.6198e-04 - last_time_step_mse: 6.8199e-04 - val_loss: 6.4746e-04 - val_last_time_step_mse: 6.7216e-04\n",
      "Epoch 19/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.5983e-04 - last_time_step_mse: 6.8043e-04\n",
      "Epoch 00019: val_last_time_step_mse improved from 0.00067 to 0.00067, saving model to Checkpoint/Wavenet(90_90_1)(filter:180)(plus).h5\n",
      "2058/2058 [==============================] - 94s 45ms/step - loss: 6.5983e-04 - last_time_step_mse: 6.8043e-04 - val_loss: 6.4704e-04 - val_last_time_step_mse: 6.7072e-04\n",
      "Epoch 20/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.5537e-04 - last_time_step_mse: 6.7521e-04\n",
      "Epoch 00020: val_last_time_step_mse improved from 0.00067 to 0.00067, saving model to Checkpoint/Wavenet(90_90_1)(filter:180)(plus).h5\n",
      "2058/2058 [==============================] - 91s 44ms/step - loss: 6.5531e-04 - last_time_step_mse: 6.7512e-04 - val_loss: 6.4628e-04 - val_last_time_step_mse: 6.7015e-04\n",
      "Epoch 21/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.5598e-04 - last_time_step_mse: 6.7595e-04\n",
      "Epoch 00021: val_last_time_step_mse improved from 0.00067 to 0.00066, saving model to Checkpoint/Wavenet(90_90_1)(filter:180)(plus).h5\n",
      "2058/2058 [==============================] - 91s 44ms/step - loss: 6.5595e-04 - last_time_step_mse: 6.7596e-04 - val_loss: 6.3867e-04 - val_last_time_step_mse: 6.6267e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.5203e-04 - last_time_step_mse: 6.7078e-04\n",
      "Epoch 00022: val_last_time_step_mse did not improve from 0.00066\n",
      "2058/2058 [==============================] - 89s 43ms/step - loss: 6.5203e-04 - last_time_step_mse: 6.7078e-04 - val_loss: 6.4551e-04 - val_last_time_step_mse: 6.6738e-04\n",
      "Epoch 23/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.5051e-04 - last_time_step_mse: 6.6910e-04\n",
      "Epoch 00023: val_last_time_step_mse did not improve from 0.00066\n",
      "2058/2058 [==============================] - 90s 44ms/step - loss: 6.5048e-04 - last_time_step_mse: 6.6906e-04 - val_loss: 6.4729e-04 - val_last_time_step_mse: 6.6968e-04\n",
      "Epoch 24/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.5329e-04 - last_time_step_mse: 6.7282e-04\n",
      "Epoch 00024: val_last_time_step_mse did not improve from 0.00066\n",
      "2058/2058 [==============================] - 90s 44ms/step - loss: 6.5329e-04 - last_time_step_mse: 6.7282e-04 - val_loss: 6.4044e-04 - val_last_time_step_mse: 6.6393e-04\n",
      "Epoch 25/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.4940e-04 - last_time_step_mse: 6.6825e-04\n",
      "Epoch 00025: val_last_time_step_mse did not improve from 0.00066\n",
      "2058/2058 [==============================] - 90s 44ms/step - loss: 6.4942e-04 - last_time_step_mse: 6.6822e-04 - val_loss: 6.5310e-04 - val_last_time_step_mse: 6.7682e-04\n",
      "Epoch 26/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.4617e-04 - last_time_step_mse: 6.6460e-04\n",
      "Epoch 00026: val_last_time_step_mse did not improve from 0.00066\n",
      "2058/2058 [==============================] - 92s 45ms/step - loss: 6.4655e-04 - last_time_step_mse: 6.6496e-04 - val_loss: 7.2721e-04 - val_last_time_step_mse: 7.4884e-04\n",
      "Epoch 27/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.4626e-04 - last_time_step_mse: 6.6521e-04\n",
      "Epoch 00027: val_last_time_step_mse improved from 0.00066 to 0.00065, saving model to Checkpoint/Wavenet(90_90_1)(filter:180)(plus).h5\n",
      "2058/2058 [==============================] - 92s 45ms/step - loss: 6.4616e-04 - last_time_step_mse: 6.6510e-04 - val_loss: 6.2852e-04 - val_last_time_step_mse: 6.5044e-04\n",
      "Epoch 28/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.4539e-04 - last_time_step_mse: 6.6426e-04\n",
      "Epoch 00028: val_last_time_step_mse did not improve from 0.00065\n",
      "2058/2058 [==============================] - 91s 44ms/step - loss: 6.4539e-04 - last_time_step_mse: 6.6426e-04 - val_loss: 6.3977e-04 - val_last_time_step_mse: 6.6103e-04\n",
      "Epoch 29/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.4345e-04 - last_time_step_mse: 6.6258e-04\n",
      "Epoch 00029: val_last_time_step_mse did not improve from 0.00065\n",
      "2058/2058 [==============================] - 109s 53ms/step - loss: 6.4345e-04 - last_time_step_mse: 6.6258e-04 - val_loss: 6.4761e-04 - val_last_time_step_mse: 6.7278e-04\n",
      "Epoch 30/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.4097e-04 - last_time_step_mse: 6.5912e-04\n",
      "Epoch 00030: val_last_time_step_mse improved from 0.00065 to 0.00064, saving model to Checkpoint/Wavenet(90_90_1)(filter:180)(plus).h5\n",
      "2058/2058 [==============================] - 116s 56ms/step - loss: 6.4097e-04 - last_time_step_mse: 6.5912e-04 - val_loss: 6.2374e-04 - val_last_time_step_mse: 6.4442e-04\n",
      "Epoch 31/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.4382e-04 - last_time_step_mse: 6.6256e-04\n",
      "Epoch 00031: val_last_time_step_mse did not improve from 0.00064\n",
      "2058/2058 [==============================] - 120s 58ms/step - loss: 6.4372e-04 - last_time_step_mse: 6.6245e-04 - val_loss: 6.3987e-04 - val_last_time_step_mse: 6.6299e-04\n",
      "Epoch 32/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.4063e-04 - last_time_step_mse: 6.5923e-04\n",
      "Epoch 00032: val_last_time_step_mse did not improve from 0.00064\n",
      "2058/2058 [==============================] - 123s 60ms/step - loss: 6.4066e-04 - last_time_step_mse: 6.5929e-04 - val_loss: 6.3794e-04 - val_last_time_step_mse: 6.5811e-04\n",
      "Epoch 33/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.3868e-04 - last_time_step_mse: 6.5728e-04\n",
      "Epoch 00033: val_last_time_step_mse did not improve from 0.00064\n",
      "2058/2058 [==============================] - 91s 44ms/step - loss: 6.3868e-04 - last_time_step_mse: 6.5728e-04 - val_loss: 6.2559e-04 - val_last_time_step_mse: 6.4763e-04\n",
      "Epoch 34/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.3847e-04 - last_time_step_mse: 6.5614e-04\n",
      "Epoch 00034: val_last_time_step_mse did not improve from 0.00064\n",
      "2058/2058 [==============================] - 91s 44ms/step - loss: 6.3836e-04 - last_time_step_mse: 6.5604e-04 - val_loss: 6.3504e-04 - val_last_time_step_mse: 6.5832e-04\n",
      "Epoch 35/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.3850e-04 - last_time_step_mse: 6.5606e-04\n",
      "Epoch 00035: val_last_time_step_mse did not improve from 0.00064\n",
      "2058/2058 [==============================] - 91s 44ms/step - loss: 6.3840e-04 - last_time_step_mse: 6.5593e-04 - val_loss: 6.2612e-04 - val_last_time_step_mse: 6.4735e-04\n",
      "Epoch 36/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.3578e-04 - last_time_step_mse: 6.5390e-04\n",
      "Epoch 00036: val_last_time_step_mse did not improve from 0.00064\n",
      "2058/2058 [==============================] - 93s 45ms/step - loss: 6.3578e-04 - last_time_step_mse: 6.5390e-04 - val_loss: 6.4473e-04 - val_last_time_step_mse: 6.6842e-04\n",
      "Epoch 37/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.3381e-04 - last_time_step_mse: 6.5153e-04\n",
      "Epoch 00037: val_last_time_step_mse did not improve from 0.00064\n",
      "2058/2058 [==============================] - 91s 44ms/step - loss: 6.3381e-04 - last_time_step_mse: 6.5153e-04 - val_loss: 6.6362e-04 - val_last_time_step_mse: 6.8591e-04\n",
      "Epoch 38/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.3546e-04 - last_time_step_mse: 6.5282e-04\n",
      "Epoch 00038: val_last_time_step_mse did not improve from 0.00064\n",
      "2058/2058 [==============================] - 90s 44ms/step - loss: 6.3546e-04 - last_time_step_mse: 6.5282e-04 - val_loss: 6.4611e-04 - val_last_time_step_mse: 6.6857e-04\n",
      "Epoch 39/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.3477e-04 - last_time_step_mse: 6.5202e-04\n",
      "Epoch 00039: val_last_time_step_mse did not improve from 0.00064\n",
      "2058/2058 [==============================] - 104s 50ms/step - loss: 6.3477e-04 - last_time_step_mse: 6.5202e-04 - val_loss: 6.2597e-04 - val_last_time_step_mse: 6.4544e-04\n",
      "Epoch 40/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.3340e-04 - last_time_step_mse: 6.5065e-04\n",
      "Epoch 00040: val_last_time_step_mse did not improve from 0.00064\n",
      "2058/2058 [==============================] - 99s 48ms/step - loss: 6.3343e-04 - last_time_step_mse: 6.5065e-04 - val_loss: 6.2927e-04 - val_last_time_step_mse: 6.5018e-04\n"
     ]
    }
   ],
   "source": [
    "history2=loaded_model.fit(X_train_scaled, y_train_scaled, epochs=50,\n",
    "                    validation_data=(X_valid_scaled,y_valid_scalebd),callbacks=callback_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history2.history['last_time_step_mse'], label='train_loss')\n",
    "plt.plot(history2.history['val_last_time_step_mse'], label='val_loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.3185e-04 - last_time_step_mse: 6.4902e-04\n",
      "Epoch 00001: val_last_time_step_mse improved from inf to 0.00064, saving model to Checkpoint/Wavenet(90_90_1)(filter:180)(plus2).h5\n",
      "2058/2058 [==============================] - 95s 46ms/step - loss: 6.3185e-04 - last_time_step_mse: 6.4902e-04 - val_loss: 6.1571e-04 - val_last_time_step_mse: 6.3639e-04\n",
      "Epoch 2/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.3208e-04 - last_time_step_mse: 6.4917e-04\n",
      "Epoch 00002: val_last_time_step_mse improved from 0.00064 to 0.00063, saving model to Checkpoint/Wavenet(90_90_1)(filter:180)(plus2).h5\n",
      "2058/2058 [==============================] - 94s 45ms/step - loss: 6.3208e-04 - last_time_step_mse: 6.4917e-04 - val_loss: 6.1235e-04 - val_last_time_step_mse: 6.3479e-04\n",
      "Epoch 3/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.2877e-04 - last_time_step_mse: 6.4576e-04\n",
      "Epoch 00003: val_last_time_step_mse did not improve from 0.00063\n",
      "2058/2058 [==============================] - 102s 49ms/step - loss: 6.2877e-04 - last_time_step_mse: 6.4576e-04 - val_loss: 6.2134e-04 - val_last_time_step_mse: 6.4443e-04\n",
      "Epoch 4/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.3489e-04 - last_time_step_mse: 6.5310e-04\n",
      "Epoch 00004: val_last_time_step_mse did not improve from 0.00063\n",
      "2058/2058 [==============================] - 89s 43ms/step - loss: 6.3538e-04 - last_time_step_mse: 6.5357e-04 - val_loss: 6.1543e-04 - val_last_time_step_mse: 6.3542e-04\n",
      "Epoch 5/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.2815e-04 - last_time_step_mse: 6.4405e-04\n",
      "Epoch 00005: val_last_time_step_mse did not improve from 0.00063\n",
      "2058/2058 [==============================] - 89s 43ms/step - loss: 6.2808e-04 - last_time_step_mse: 6.4397e-04 - val_loss: 6.6611e-04 - val_last_time_step_mse: 6.8812e-04\n",
      "Epoch 6/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.2867e-04 - last_time_step_mse: 6.4667e-04\n",
      "Epoch 00006: val_last_time_step_mse did not improve from 0.00063\n",
      "2058/2058 [==============================] - 90s 44ms/step - loss: 6.2856e-04 - last_time_step_mse: 6.4655e-04 - val_loss: 6.2136e-04 - val_last_time_step_mse: 6.4349e-04\n",
      "Epoch 7/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.2464e-04 - last_time_step_mse: 6.4225e-04\n",
      "Epoch 00007: val_last_time_step_mse did not improve from 0.00063\n",
      "2058/2058 [==============================] - 97s 47ms/step - loss: 6.2464e-04 - last_time_step_mse: 6.4225e-04 - val_loss: 6.3656e-04 - val_last_time_step_mse: 6.5971e-04\n",
      "Epoch 8/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.2624e-04 - last_time_step_mse: 6.4399e-04\n",
      "Epoch 00008: val_last_time_step_mse did not improve from 0.00063\n",
      "2058/2058 [==============================] - 128s 62ms/step - loss: 6.2613e-04 - last_time_step_mse: 6.4387e-04 - val_loss: 6.1854e-04 - val_last_time_step_mse: 6.3978e-04\n",
      "Epoch 9/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.2494e-04 - last_time_step_mse: 6.4147e-04\n",
      "Epoch 00009: val_last_time_step_mse did not improve from 0.00063\n",
      "2058/2058 [==============================] - 123s 60ms/step - loss: 6.2494e-04 - last_time_step_mse: 6.4147e-04 - val_loss: 6.1975e-04 - val_last_time_step_mse: 6.4070e-04\n",
      "Epoch 10/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.2742e-04 - last_time_step_mse: 6.4433e-04\n",
      "Epoch 00010: val_last_time_step_mse did not improve from 0.00063\n",
      "2058/2058 [==============================] - 131s 64ms/step - loss: 6.2742e-04 - last_time_step_mse: 6.4433e-04 - val_loss: 6.2049e-04 - val_last_time_step_mse: 6.4180e-04\n",
      "Epoch 11/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.2541e-04 - last_time_step_mse: 6.4214e-04\n",
      "Epoch 00011: val_last_time_step_mse did not improve from 0.00063\n",
      "2058/2058 [==============================] - 129s 63ms/step - loss: 6.2541e-04 - last_time_step_mse: 6.4214e-04 - val_loss: 6.4341e-04 - val_last_time_step_mse: 6.6508e-04\n",
      "Epoch 12/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.2365e-04 - last_time_step_mse: 6.4015e-04\n",
      "Epoch 00012: val_last_time_step_mse did not improve from 0.00063\n",
      "2058/2058 [==============================] - 119s 58ms/step - loss: 6.2358e-04 - last_time_step_mse: 6.4006e-04 - val_loss: 6.1998e-04 - val_last_time_step_mse: 6.4320e-04\n",
      "Epoch 13/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.2088e-04 - last_time_step_mse: 6.3765e-04\n",
      "Epoch 00013: val_last_time_step_mse did not improve from 0.00063\n",
      "2058/2058 [==============================] - 123s 60ms/step - loss: 6.2088e-04 - last_time_step_mse: 6.3765e-04 - val_loss: 6.1737e-04 - val_last_time_step_mse: 6.3695e-04\n",
      "Epoch 14/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.2527e-04 - last_time_step_mse: 6.4194e-04\n",
      "Epoch 00014: val_last_time_step_mse did not improve from 0.00063\n",
      "2058/2058 [==============================] - 117s 57ms/step - loss: 6.2520e-04 - last_time_step_mse: 6.4185e-04 - val_loss: 6.3036e-04 - val_last_time_step_mse: 6.5015e-04\n",
      "Epoch 15/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.2206e-04 - last_time_step_mse: 6.3905e-04\n",
      "Epoch 00015: val_last_time_step_mse did not improve from 0.00063\n",
      "2058/2058 [==============================] - 126s 61ms/step - loss: 6.2200e-04 - last_time_step_mse: 6.3899e-04 - val_loss: 6.2863e-04 - val_last_time_step_mse: 6.4881e-04\n",
      "Epoch 16/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.1926e-04 - last_time_step_mse: 6.3593e-04\n",
      "Epoch 00016: val_last_time_step_mse did not improve from 0.00063\n",
      "2058/2058 [==============================] - 126s 61ms/step - loss: 6.1926e-04 - last_time_step_mse: 6.3593e-04 - val_loss: 6.2197e-04 - val_last_time_step_mse: 6.4051e-04\n",
      "Epoch 17/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.1848e-04 - last_time_step_mse: 6.3472e-04\n",
      "Epoch 00017: val_last_time_step_mse improved from 0.00063 to 0.00062, saving model to Checkpoint/Wavenet(90_90_1)(filter:180)(plus2).h5\n",
      "2058/2058 [==============================] - 131s 64ms/step - loss: 6.1844e-04 - last_time_step_mse: 6.3468e-04 - val_loss: 6.0631e-04 - val_last_time_step_mse: 6.2490e-04\n",
      "Epoch 18/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.1703e-04 - last_time_step_mse: 6.3376e-04\n",
      "Epoch 00018: val_last_time_step_mse did not improve from 0.00062\n",
      "2058/2058 [==============================] - 121s 59ms/step - loss: 6.1703e-04 - last_time_step_mse: 6.3376e-04 - val_loss: 6.2625e-04 - val_last_time_step_mse: 6.4496e-04\n",
      "Epoch 19/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.1879e-04 - last_time_step_mse: 6.3547e-04\n",
      "Epoch 00019: val_last_time_step_mse did not improve from 0.00062\n",
      "2058/2058 [==============================] - 126s 61ms/step - loss: 6.1879e-04 - last_time_step_mse: 6.3547e-04 - val_loss: 6.1967e-04 - val_last_time_step_mse: 6.3967e-04\n",
      "Epoch 20/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.1858e-04 - last_time_step_mse: 6.3606e-04\n",
      "Epoch 00020: val_last_time_step_mse did not improve from 0.00062\n",
      "2058/2058 [==============================] - 122s 59ms/step - loss: 6.1854e-04 - last_time_step_mse: 6.3600e-04 - val_loss: 6.1097e-04 - val_last_time_step_mse: 6.3065e-04\n",
      "Epoch 21/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.1731e-04 - last_time_step_mse: 6.3319e-04\n",
      "Epoch 00021: val_last_time_step_mse improved from 0.00062 to 0.00062, saving model to Checkpoint/Wavenet(90_90_1)(filter:180)(plus2).h5\n",
      "2058/2058 [==============================] - 123s 60ms/step - loss: 6.1731e-04 - last_time_step_mse: 6.3319e-04 - val_loss: 6.0023e-04 - val_last_time_step_mse: 6.2018e-04\n",
      "Epoch 22/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.1713e-04 - last_time_step_mse: 6.3450e-04\n",
      "Epoch 00022: val_last_time_step_mse improved from 0.00062 to 0.00062, saving model to Checkpoint/Wavenet(90_90_1)(filter:180)(plus2).h5\n",
      "2058/2058 [==============================] - 123s 60ms/step - loss: 6.1721e-04 - last_time_step_mse: 6.3460e-04 - val_loss: 5.9707e-04 - val_last_time_step_mse: 6.1585e-04\n",
      "Epoch 23/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.1526e-04 - last_time_step_mse: 6.3161e-04\n",
      "Epoch 00023: val_last_time_step_mse did not improve from 0.00062\n",
      "2058/2058 [==============================] - 123s 60ms/step - loss: 6.1555e-04 - last_time_step_mse: 6.3192e-04 - val_loss: 6.0118e-04 - val_last_time_step_mse: 6.2171e-04\n",
      "Epoch 24/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.1662e-04 - last_time_step_mse: 6.3342e-04\n",
      "Epoch 00024: val_last_time_step_mse did not improve from 0.00062\n",
      "2058/2058 [==============================] - 125s 61ms/step - loss: 6.1662e-04 - last_time_step_mse: 6.3342e-04 - val_loss: 6.0799e-04 - val_last_time_step_mse: 6.2691e-04\n",
      "Epoch 25/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.1746e-04 - last_time_step_mse: 6.3468e-04\n",
      "Epoch 00025: val_last_time_step_mse did not improve from 0.00062\n",
      "2058/2058 [==============================] - 120s 58ms/step - loss: 6.1746e-04 - last_time_step_mse: 6.3468e-04 - val_loss: 6.0598e-04 - val_last_time_step_mse: 6.2602e-04\n",
      "Epoch 26/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.2034e-04 - last_time_step_mse: 6.3751e-04\n",
      "Epoch 00026: val_last_time_step_mse did not improve from 0.00062\n",
      "2058/2058 [==============================] - 121s 59ms/step - loss: 6.2038e-04 - last_time_step_mse: 6.3747e-04 - val_loss: 6.1142e-04 - val_last_time_step_mse: 6.3214e-04\n",
      "Epoch 27/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.1354e-04 - last_time_step_mse: 6.3088e-04\n",
      "Epoch 00027: val_last_time_step_mse did not improve from 0.00062\n",
      "2058/2058 [==============================] - 122s 59ms/step - loss: 6.1354e-04 - last_time_step_mse: 6.3087e-04 - val_loss: 6.2110e-04 - val_last_time_step_mse: 6.3667e-04\n",
      "Epoch 28/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.1601e-04 - last_time_step_mse: 6.3273e-04\n",
      "Epoch 00028: val_last_time_step_mse did not improve from 0.00062\n",
      "2058/2058 [==============================] - 126s 61ms/step - loss: 6.1601e-04 - last_time_step_mse: 6.3267e-04 - val_loss: 6.0052e-04 - val_last_time_step_mse: 6.1996e-04\n",
      "Epoch 29/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.1493e-04 - last_time_step_mse: 6.3259e-04\n",
      "Epoch 00029: val_last_time_step_mse did not improve from 0.00062\n",
      "2058/2058 [==============================] - 131s 64ms/step - loss: 6.1493e-04 - last_time_step_mse: 6.3259e-04 - val_loss: 6.0537e-04 - val_last_time_step_mse: 6.2549e-04\n",
      "Epoch 30/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.1134e-04 - last_time_step_mse: 6.2762e-04\n",
      "Epoch 00030: val_last_time_step_mse improved from 0.00062 to 0.00062, saving model to Checkpoint/Wavenet(90_90_1)(filter:180)(plus2).h5\n",
      "2058/2058 [==============================] - 122s 59ms/step - loss: 6.1163e-04 - last_time_step_mse: 6.2788e-04 - val_loss: 5.9708e-04 - val_last_time_step_mse: 6.1507e-04\n",
      "Epoch 31/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.1330e-04 - last_time_step_mse: 6.3016e-04\n",
      "Epoch 00031: val_last_time_step_mse did not improve from 0.00062\n",
      "2058/2058 [==============================] - 130s 63ms/step - loss: 6.1330e-04 - last_time_step_mse: 6.3016e-04 - val_loss: 6.0555e-04 - val_last_time_step_mse: 6.2497e-04\n",
      "Epoch 32/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.1267e-04 - last_time_step_mse: 6.2887e-04\n",
      "Epoch 00032: val_last_time_step_mse did not improve from 0.00062\n",
      "2058/2058 [==============================] - 124s 60ms/step - loss: 6.1265e-04 - last_time_step_mse: 6.2882e-04 - val_loss: 6.0247e-04 - val_last_time_step_mse: 6.2329e-04\n",
      "Epoch 33/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.1043e-04 - last_time_step_mse: 6.2768e-04\n",
      "Epoch 00033: val_last_time_step_mse did not improve from 0.00062\n",
      "2058/2058 [==============================] - 124s 60ms/step - loss: 6.1043e-04 - last_time_step_mse: 6.2765e-04 - val_loss: 6.0171e-04 - val_last_time_step_mse: 6.2217e-04\n",
      "Epoch 34/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.0974e-04 - last_time_step_mse: 6.2682e-04\n",
      "Epoch 00034: val_last_time_step_mse did not improve from 0.00062\n",
      "2058/2058 [==============================] - 122s 59ms/step - loss: 6.0974e-04 - last_time_step_mse: 6.2682e-04 - val_loss: 6.0272e-04 - val_last_time_step_mse: 6.1976e-04\n",
      "Epoch 35/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.1154e-04 - last_time_step_mse: 6.2916e-04\n",
      "Epoch 00035: val_last_time_step_mse did not improve from 0.00062\n",
      "2058/2058 [==============================] - 124s 60ms/step - loss: 6.1154e-04 - last_time_step_mse: 6.2916e-04 - val_loss: 6.0726e-04 - val_last_time_step_mse: 6.2659e-04\n",
      "Epoch 36/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.0880e-04 - last_time_step_mse: 6.2523e-04\n",
      "Epoch 00036: val_last_time_step_mse did not improve from 0.00062\n",
      "2058/2058 [==============================] - 124s 60ms/step - loss: 6.0880e-04 - last_time_step_mse: 6.2523e-04 - val_loss: 6.1380e-04 - val_last_time_step_mse: 6.3517e-04\n",
      "Epoch 37/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.0903e-04 - last_time_step_mse: 6.2639e-04\n",
      "Epoch 00037: val_last_time_step_mse did not improve from 0.00062\n",
      "2058/2058 [==============================] - 131s 64ms/step - loss: 6.0903e-04 - last_time_step_mse: 6.2639e-04 - val_loss: 6.0840e-04 - val_last_time_step_mse: 6.2689e-04\n",
      "Epoch 38/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.0965e-04 - last_time_step_mse: 6.2610e-04\n",
      "Epoch 00038: val_last_time_step_mse did not improve from 0.00062\n",
      "2058/2058 [==============================] - 126s 61ms/step - loss: 6.0971e-04 - last_time_step_mse: 6.2612e-04 - val_loss: 6.0598e-04 - val_last_time_step_mse: 6.2751e-04\n",
      "Epoch 39/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.0833e-04 - last_time_step_mse: 6.2542e-04\n",
      "Epoch 00039: val_last_time_step_mse improved from 0.00062 to 0.00061, saving model to Checkpoint/Wavenet(90_90_1)(filter:180)(plus2).h5\n",
      "2058/2058 [==============================] - 124s 60ms/step - loss: 6.0833e-04 - last_time_step_mse: 6.2542e-04 - val_loss: 5.9542e-04 - val_last_time_step_mse: 6.1323e-04\n",
      "Epoch 40/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.0614e-04 - last_time_step_mse: 6.2345e-04\n",
      "Epoch 00040: val_last_time_step_mse did not improve from 0.00061\n",
      "2058/2058 [==============================] - 132s 64ms/step - loss: 6.0611e-04 - last_time_step_mse: 6.2339e-04 - val_loss: 6.1259e-04 - val_last_time_step_mse: 6.3502e-04\n",
      "Epoch 41/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.0928e-04 - last_time_step_mse: 6.2604e-04\n",
      "Epoch 00041: val_last_time_step_mse did not improve from 0.00061\n",
      "2058/2058 [==============================] - 126s 61ms/step - loss: 6.0917e-04 - last_time_step_mse: 6.2595e-04 - val_loss: 6.0544e-04 - val_last_time_step_mse: 6.2815e-04\n",
      "Epoch 42/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.0997e-04 - last_time_step_mse: 6.2635e-04\n",
      "Epoch 00042: val_last_time_step_mse did not improve from 0.00061\n",
      "2058/2058 [==============================] - 135s 66ms/step - loss: 6.0989e-04 - last_time_step_mse: 6.2629e-04 - val_loss: 5.9989e-04 - val_last_time_step_mse: 6.1932e-04\n",
      "Epoch 43/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.0541e-04 - last_time_step_mse: 6.2278e-04\n",
      "Epoch 00043: val_last_time_step_mse did not improve from 0.00061\n",
      "2058/2058 [==============================] - 124s 60ms/step - loss: 6.0542e-04 - last_time_step_mse: 6.2279e-04 - val_loss: 6.0231e-04 - val_last_time_step_mse: 6.2229e-04\n",
      "Epoch 44/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.0778e-04 - last_time_step_mse: 6.2492e-04\n",
      "Epoch 00044: val_last_time_step_mse did not improve from 0.00061\n",
      "2058/2058 [==============================] - 130s 63ms/step - loss: 6.0776e-04 - last_time_step_mse: 6.2488e-04 - val_loss: 5.9596e-04 - val_last_time_step_mse: 6.1444e-04\n",
      "Epoch 45/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.0870e-04 - last_time_step_mse: 6.2606e-04\n",
      "Epoch 00045: val_last_time_step_mse did not improve from 0.00061\n",
      "2058/2058 [==============================] - 130s 63ms/step - loss: 6.0870e-04 - last_time_step_mse: 6.2606e-04 - val_loss: 5.9559e-04 - val_last_time_step_mse: 6.1417e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.0605e-04 - last_time_step_mse: 6.2286e-04\n",
      "Epoch 00046: val_last_time_step_mse did not improve from 0.00061\n",
      "2058/2058 [==============================] - 131s 64ms/step - loss: 6.0605e-04 - last_time_step_mse: 6.2286e-04 - val_loss: 6.1936e-04 - val_last_time_step_mse: 6.3851e-04\n",
      "Epoch 47/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.0552e-04 - last_time_step_mse: 6.2208e-04\n",
      "Epoch 00047: val_last_time_step_mse did not improve from 0.00061\n",
      "2058/2058 [==============================] - 115s 56ms/step - loss: 6.0566e-04 - last_time_step_mse: 6.2219e-04 - val_loss: 6.0185e-04 - val_last_time_step_mse: 6.2026e-04\n",
      "Epoch 48/50\n",
      "2058/2058 [==============================] - ETA: 0s - loss: 6.0588e-04 - last_time_step_mse: 6.2308e-04\n",
      "Epoch 00048: val_last_time_step_mse did not improve from 0.00061\n",
      "2058/2058 [==============================] - 107s 52ms/step - loss: 6.0588e-04 - last_time_step_mse: 6.2308e-04 - val_loss: 6.0173e-04 - val_last_time_step_mse: 6.2326e-04\n",
      "Epoch 49/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.0617e-04 - last_time_step_mse: 6.2446e-04\n",
      "Epoch 00049: val_last_time_step_mse improved from 0.00061 to 0.00061, saving model to Checkpoint/Wavenet(90_90_1)(filter:180)(plus2).h5\n",
      "2058/2058 [==============================] - 106s 52ms/step - loss: 6.0643e-04 - last_time_step_mse: 6.2473e-04 - val_loss: 5.8983e-04 - val_last_time_step_mse: 6.0865e-04\n",
      "Epoch 50/50\n",
      "2057/2058 [============================>.] - ETA: 0s - loss: 6.0336e-04 - last_time_step_mse: 6.2107e-04\n",
      "Epoch 00050: val_last_time_step_mse did not improve from 0.00061\n",
      "2058/2058 [==============================] - 116s 56ms/step - loss: 6.0344e-04 - last_time_step_mse: 6.2112e-04 - val_loss: 5.9944e-04 - val_last_time_step_mse: 6.1949e-04\n"
     ]
    }
   ],
   "source": [
    "history3=loaded_model.fit(X_train_scaled, y_train_scaled, epochs=50,\n",
    "                    validation_data=(X_valid_scaled,y_valid_scaled),callbacks=callback_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvGRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='ConvGRU(90_90_1).h5'\n",
    "callback_list1 = [tf.keras.callbacks.ModelCheckpoint(filepath='Checkpoint/{}'.format(filename),\n",
    "                                                    monitor='val_last_time_step_mse',\n",
    "                                                    verbose=1,\n",
    "                                                    save_best_only=True,\n",
    "                                                    mode='min'),\n",
    "                 tf.keras.callbacks.EarlyStopping(monitor='val_last_time_step_mse',\n",
    "                                                  patience=18)]\n",
    "\n",
    "model= keras.models.Sequential([\n",
    "    keras.layers.Conv1D(filters=180, kernel_size=4, strides=3, padding=\"valid\",\n",
    "                        input_shape=[None, 17]),\n",
    "    #-------뭐라도 해보기 ----- OK, !! \n",
    "    #keras.layers.MaxPooling1D(pool_size=1, strides=None),\n",
    "    keras.layers.GRU(180, return_sequences=True),\n",
    "    keras.layers.GRU(180, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(90))\n",
    "    ,\n",
    "])\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"mae\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
    "\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train_scaled[:, 3::3], epochs=80,\n",
    "                    validation_data=(X_valid_scaled,y_valid_scaled[:, 3::3]),callbacks=callback_list1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cancer",
   "language": "python",
   "name": "cancer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
